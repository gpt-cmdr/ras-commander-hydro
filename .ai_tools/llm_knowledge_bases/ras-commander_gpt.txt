Project Structure (files included):
├── .gitattributes
├── .gitignore
├── GEODATABASE_NAMING_CONVENTIONS.md
├── INSTALL.md
├── LICENSE
├── RASCommander_Help.html
├── README_Help.md
├── Resources
│   ├── Customize tool behavior in a Python toolbox—ArcGIS Pro - Documentation.url
│   ├── ESRI Developer Links.txt
│   └── Esri Developer.url
├── Scripts
│   └── archydro
│       ├── New Naming Notes.txt
│       ├── rc_load_hecras_1d_geometry.py
│       ├── rc_load_hecras_2d_geometry.py
│       ├── rc_load_hecras_2d_results.py
│       ├── rc_load_ras_terrain.py
│       ├── rc_organize_ras_project.py
│       ├── rc_organize_ras_project.py.backup
│       └── rc_utils.py
├── TRADEMARKS.md
├── install_toolbox.ps1
├── testdata
│   └── Test Data Notes.txt
├── toolboxes
│   ├── RAS Commander.LoadHECRAS1DGeometry.pyt.xml
│   ├── RAS Commander.LoadHECRAS2DGeometry.pyt.xml
│   ├── RAS Commander.LoadHECRAS2DResults.pyt.xml
│   ├── RAS Commander.LoadRASTerrain.pyt.xml
│   ├── RAS Commander.OrganizeRASProject.pyt.xml
│   ├── RAS Commander.pyt
│   └── RAS Commander.pyt.xml
└── uninstall_toolbox.ps1

File: c:\GH\ras-commander-hydro\.gitattributes
==================================================
# Auto detect text files and perform LF normalization
* text=auto

==================================================

File: c:\GH\ras-commander-hydro\.gitignore
==================================================
# Test data
testdata/

# Python cache
__pycache__/
**/__pycache__/
*.pyc
*.pyo
*.pyd
.Python
==================================================

File: c:\GH\ras-commander-hydro\GEODATABASE_NAMING_CONVENTIONS.md
==================================================
# HEC-RAS Geodatabase Naming Conventions

## Overview
This document defines the naming conventions used by the RAS Commander toolbox for organizing HEC-RAS data in geodatabases.

## Geodatabase Structure

### Geodatabase Name
- Pattern: `{ProjectName}.p{PlanNumber}.gdb`
- Example: `BaldEagleDamBrk.p07.gdb`

### Feature Dataset Organization
Each plan file creates a single feature dataset:
- Pattern: `{ProjectName}_Plan{PlanNumber}`
- Example: `BaldEagleDamBrk_Plan07`

### Feature Class Naming
Within each feature dataset:

#### 1D Geometry
- `CrossSections_{ProjectName}_Plan{PlanNumber}`
- `RiverCenterlines_{ProjectName}_Plan{PlanNumber}`
- `BankLines_{ProjectName}_Plan{PlanNumber}`
- `EdgeLines_{ProjectName}_Plan{PlanNumber}`
- `Structures1D_{ProjectName}_Plan{PlanNumber}`

#### 2D Geometry
- `Breaklines2D_{ProjectName}_Plan{PlanNumber}`
- `BCLines2D_{ProjectName}_Plan{PlanNumber}`
- `MeshPerimeters_{ProjectName}_Plan{PlanNumber}`
- `MeshCellCenters_{ProjectName}_Plan{PlanNumber}`
- `MeshCellFaces_{ProjectName}_Plan{PlanNumber}`
- `MeshCellPolygons_{ProjectName}_Plan{PlanNumber}`

#### Pipe Networks
- `PipeConduits_{ProjectName}_Plan{PlanNumber}`
- `PipeNodes_{ProjectName}_Plan{PlanNumber}`
- `PipeNetworks_{ProjectName}_Plan{PlanNumber}`

#### Results
- `MaxWSE_{ProjectName}_Plan{PlanNumber}`
- `MaxVelocity_{ProjectName}_Plan{PlanNumber}`

## Multi-Model Comparison Support
This naming convention allows multiple HEC-RAS models with different projections to coexist in the same geodatabase, as long as project names are unique.

## Important Notes
- Users should ensure project names are unique when comparing different models
- Plan numbers are extracted from the filename pattern p{XX}.hdf
- If needed, rename HDF files before importing to ensure unique project identification
==================================================

File: c:\GH\ras-commander-hydro\INSTALL.md
==================================================
# Installation Instructions for Arc Hydro RAS Commander Toolbox

## Prerequisites

- ArcGIS Pro must be installed
- Administrator privileges are required to install to Program Files

## Installation Methods

### Method 1: Using the Batch File (Easiest)

1. Double-click `install_toolbox_as_admin.bat`
2. Click "Yes" when Windows asks for administrator privileges
3. Follow the prompts in the PowerShell window

### Method 2: Using PowerShell Directly

1. Right-click on `install_toolbox.ps1`
2. Select "Run with PowerShell"
3. If prompted for administrator privileges, click "Yes"

OR

1. Open PowerShell as Administrator
2. Navigate to the repository directory
3. Run: `.\install_toolbox.ps1`

### Method 3: Manual Installation

If the scripts don't work, you can manually copy the files:

1. Copy `Scripts\ras_commander\*` to:
   `C:\Program Files\ArcGIS\Pro\Resources\ArcToolBox\Scripts\ras_commander\`

2. Copy `toolboxes\RAS Commander.pyt` to:
   `C:\Program Files\ArcGIS\Pro\Resources\ArcToolBox\toolboxes\RAS Commander.pyt`

3. (Optional) Copy layer templates, images, and geodatabase templates to their respective directories

## Development Mode

The installation script offers a "development mode" option that creates symbolic links instead of copying files. This is useful for developers because:

- Changes made in the repository are immediately reflected in ArcGIS Pro
- No need to reinstall after making changes
- Easy to switch between development and production versions

To use development mode:
1. Run the installation script
2. When prompted, type 'y' to create symlinks

Note: Creating symbolic links requires administrator privileges on Windows.

## Verifying Installation

After installation:

1. Open ArcGIS Pro
2. Go to the Catalog pane
3. Navigate to Toolboxes → System Toolboxes
4. Look for "RAS Commander"
5. The tools should be available:
   - Load HEC-RAS Terrain
   - Load HEC-RAS 6.x HDF Data

## Troubleshooting

If you encounter issues:

1. **"Script cannot be loaded because running scripts is disabled"**
   - This is a PowerShell execution policy issue
   - The batch file should bypass this, but if not, run:
     `Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser`

2. **"Access denied" errors**
   - Ensure you're running as administrator
   - Check that ArcGIS Pro is not running

3. **"ArcGIS Pro not found"**
   - Verify ArcGIS Pro is installed
   - Check if it's installed in a non-standard location

## Uninstallation

To remove the Arc Hydro RAS Commander toolbox, you have several options:

### Method 1: Using the Uninstall Batch File (Easiest)

1. Double-click `uninstall_toolbox_as_admin.bat`
2. Click "Yes" when Windows asks for administrator privileges
3. Confirm the uninstallation when prompted

### Method 2: Using PowerShell Directly

1. Right-click on `uninstall_toolbox.ps1`
2. Select "Run with PowerShell"
3. If prompted for administrator privileges, click "Yes"

OR

1. Open PowerShell as Administrator
2. Navigate to the repository directory
3. Run: `.\uninstall_toolbox.ps1`

### Method 3: Manual Uninstallation

To manually remove the toolbox:

1. Delete the following:
   - Directory: `C:\Program Files\ArcGIS\Pro\Resources\ArcToolBox\Scripts\ras_commander`
   - File: `C:\Program Files\ArcGIS\Pro\Resources\ArcToolBox\toolboxes\RAS Commander.pyt`
   - File: `C:\Program Files\ArcGIS\Pro\Resources\ArcToolBox\toolboxes\RAS Commander.pyt.xml`
   - Directory: `C:\Program Files\ArcGIS\Pro\Resources\ArcToolBox\Templates\Layers\archydro\ras-commander`
   - Directory: `C:\Program Files\ArcGIS\Pro\Resources\ArcToolBox\Data\archydro\Ras2DTemplate.gdb`

Note: The uninstaller will properly handle both regular installations and symlinked (development mode) installations.
==================================================

File: c:\GH\ras-commander-hydro\install_toolbox.ps1
==================================================
# install_toolbox.ps1
#
# Installs the Arc Hydro RAS Commander toolbox for local development.
# This script copies all toolbox files from the repository to the ArcGIS Pro installation directories.
#
# Usage:
#   Right-click on this file and select "Run with PowerShell" 
#   OR
#   Open PowerShell as Administrator and run: .\install_toolbox.ps1
#
# Note: This script requires administrator privileges to write to Program Files.

# Check if running as administrator
if (-NOT ([Security.Principal.WindowsPrincipal] [Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")) {
    Write-Host "This script requires Administrator privileges." -ForegroundColor Red
    Write-Host "Please run PowerShell as Administrator and try again." -ForegroundColor Yellow
    Write-Host ""
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 1
}

Write-Host "Arc Hydro RAS Commander Toolbox Installer" -ForegroundColor Cyan
Write-Host ("=" * 50) -ForegroundColor Cyan
Write-Host ""

# Get the script directory (repository root)
$repoRoot = Split-Path -Parent $MyInvocation.MyCommand.Definition

# Function to find ArcGIS Pro installation
function Find-ArcGISPro {
    $potentialPaths = @(
        "C:\Program Files\ArcGIS\Pro",
        "C:\Program Files (x86)\ArcGIS\Pro",
        "$env:ProgramFiles\ArcGIS\Pro",
        "${env:ProgramFiles(x86)}\ArcGIS\Pro"
    )
    
    # Check each potential path
    foreach ($path in $potentialPaths) {
        if (Test-Path $path) {
            $toolboxPath = Join-Path $path "Resources\ArcToolBox"
            if (Test-Path $toolboxPath) {
                return $path
            }
        }
    }
    
    # Check registry
    try {
        $regPath = "HKLM:\SOFTWARE\ESRI\ArcGISPro"
        if (Test-Path $regPath) {
            $installDir = (Get-ItemProperty -Path $regPath -Name InstallDir -ErrorAction SilentlyContinue).InstallDir
            if ($installDir -and (Test-Path $installDir)) {
                return $installDir
            }
        }
    } catch {
        # Registry check failed, continue
    }
    
    return $null
}

# Find ArcGIS Pro installation
$arcgisProPath = Find-ArcGISPro
if (-not $arcgisProPath) {
    Write-Host "ERROR: Could not find ArcGIS Pro installation." -ForegroundColor Red
    Write-Host "Please ensure ArcGIS Pro is installed." -ForegroundColor Yellow
    Write-Host ""
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 1
}

Write-Host "Found ArcGIS Pro at: $arcgisProPath" -ForegroundColor Green
Write-Host ""

# Define source and destination mappings
$mappings = @(
    @{
        Name = "Python Scripts"
        Source = Join-Path $repoRoot "Scripts\archydro"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\Scripts\archydro"
        Type = "Directory"
        Required = $true
        Filter = "rc_*.py"
    },
    @{
        Name = "Python Toolbox"
        Source = Join-Path $repoRoot "toolboxes\RAS Commander.pyt"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\toolboxes\RAS Commander.pyt"
        Type = "File"
        Required = $true
    },
    @{
        Name = "Layer Templates"
        Source = Join-Path $repoRoot "Templates\Layers\archydro"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\Templates\Layers\archydro"
        Type = "Directory"
        Required = $false
    },
    @{
        Name = "Images"
        Source = Join-Path $repoRoot "Images"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\Images"
        Type = "Directory"
        Required = $false
    },
    @{
        Name = "Geodatabase Template"
        Source = Join-Path $repoRoot "Data\archydro\Ras2DTemplate.gdb"
        Destination = Join-Path $arcgisProPath "Resources\ArcToolBox\Data\archydro\Ras2DTemplate.gdb"
        Type = "Directory"
        Required = $false
    }
)

Write-Host "Installing Arc Hydro RAS Commander components..." -ForegroundColor Yellow
Write-Host ""

$successCount = 0
$errorCount = 0
$skippedCount = 0

foreach ($mapping in $mappings) {
    Write-Host "Installing: $($mapping.Name)" -ForegroundColor White
    
    if (-not (Test-Path $mapping.Source)) {
        if ($mapping.Required) {
            Write-Host "  ERROR: Source not found: $($mapping.Source)" -ForegroundColor Red
            $errorCount++
        } else {
            Write-Host "  Skipping (optional component not found)" -ForegroundColor Gray
            $skippedCount++
        }
        continue
    }
    
    try {
        # Create parent directory if it doesn't exist
        $parentDir = Split-Path -Parent $mapping.Destination
        if (-not (Test-Path $parentDir)) {
            New-Item -ItemType Directory -Path $parentDir -Force | Out-Null
        }
        
        # Remove existing destination if it exists
        if (Test-Path $mapping.Destination) {
            Remove-Item -Path $mapping.Destination -Recurse -Force
        }
        
        # Copy the content
        if ($mapping.Type -eq "Directory") {
            if ($mapping.Filter) {
                # Create destination directory
                New-Item -ItemType Directory -Path $mapping.Destination -Force | Out-Null
                # Copy only filtered files
                Get-ChildItem -Path $mapping.Source -Filter $mapping.Filter | ForEach-Object {
                    Copy-Item -Path $_.FullName -Destination $mapping.Destination -Force
                }
            } else {
                Copy-Item -Path $mapping.Source -Destination $mapping.Destination -Recurse -Force
            }
        } else {
            # For files, ensure the destination directory exists
            $destDir = Split-Path -Parent $mapping.Destination
            if (-not (Test-Path $destDir)) {
                New-Item -ItemType Directory -Path $destDir -Force | Out-Null
            }
            Copy-Item -Path $mapping.Source -Destination $mapping.Destination -Force
        }
        
        Write-Host "  Success: Copied to $($mapping.Destination)" -ForegroundColor Green
        $successCount++
        
    } catch {
        Write-Host "  ERROR: Failed to copy - $_" -ForegroundColor Red
        $errorCount++
    }
}

Write-Host ""
Write-Host ("=" * 50) -ForegroundColor Cyan
Write-Host "Installation Summary:" -ForegroundColor Cyan
Write-Host "  Successful: $successCount components" -ForegroundColor Green
Write-Host "  Failed: $errorCount components" -ForegroundColor $(if ($errorCount -gt 0) { "Red" } else { "Gray" })
Write-Host "  Skipped: $skippedCount optional components" -ForegroundColor Gray
Write-Host ""

if ($errorCount -gt 0) {
    Write-Host "Some components failed to install. Please check the errors above." -ForegroundColor Yellow
} else {
    Write-Host "All components installed successfully!" -ForegroundColor Green
    Write-Host ""
    Write-Host "To use the toolbox in ArcGIS Pro:" -ForegroundColor Yellow
    Write-Host "  1. Open ArcGIS Pro"
    Write-Host "  2. Go to the Catalog pane"
    Write-Host "  3. Navigate to Toolboxes > Arc Hydro RAS Commander"
    Write-Host "  4. The tools will be available there"
}

Write-Host ""
Write-Host ("-" * 50) -ForegroundColor Gray

# Ask about development mode (symlinks)
Write-Host ""
$response = Read-Host "Would you like to create development symlinks instead of copying? (y/N)"

if ($response -eq 'y' -or $response -eq 'Y') {
    Write-Host ""
    Write-Host "Creating symlinks for development mode..." -ForegroundColor Yellow
    Write-Host "(This allows changes in the repo to be reflected immediately)" -ForegroundColor Gray
    Write-Host ""
    
    $symlinkSuccess = 0
    $symlinkError = 0
    
    foreach ($mapping in $mappings) {
        if (-not (Test-Path $mapping.Source)) {
            continue
        }
        
        Write-Host "Creating symlink: $($mapping.Name)" -ForegroundColor White
        
        try {
            # Remove existing destination if it exists
            if (Test-Path $mapping.Destination) {
                Remove-Item -Path $mapping.Destination -Recurse -Force
            }
            
            # Create parent directory if needed
            $parentDir = Split-Path -Parent $mapping.Destination
            if (-not (Test-Path $parentDir)) {
                New-Item -ItemType Directory -Path $parentDir -Force | Out-Null
            }
            
            # Create symlink
            New-Item -ItemType SymbolicLink -Path $mapping.Destination -Target $mapping.Source | Out-Null
            
            Write-Host "  Success: Symlinked to $($mapping.Destination)" -ForegroundColor Green
            $symlinkSuccess++
            
        } catch {
            Write-Host "  ERROR: Failed to create symlink - $_" -ForegroundColor Red
            $symlinkError++
        }
    }
    
    Write-Host ""
    Write-Host "Symlink Summary:" -ForegroundColor Cyan
    Write-Host "  Successful: $symlinkSuccess symlinks" -ForegroundColor Green
    Write-Host "  Failed: $symlinkError symlinks" -ForegroundColor $(if ($symlinkError -gt 0) { "Red" } else { "Gray" })
}

Write-Host ""
Write-Host "Press any key to exit..."
$null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
==================================================

File: c:\GH\ras-commander-hydro\LICENSE
==================================================
The MIT License (MIT)
Copyright © CLB Engineering Corporation

Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


NOTICE
* **HEC‑RAS**™ is a trademark of the U.S. Army Corps of Engineers (USACE) Hydrologic Engineering Center (HEC).
* **ARC HYDRO** is a trademark of Environmental Systems Research Institute (ESRI)

"RAS Commander" and "RAS Commander Arc Hydro Tools" are independent open source projects and are **not** affiliated with, endorsed by, or sponsored by USACE or HEC.
==================================================

File: c:\GH\ras-commander-hydro\RASCommander_Help.html
==================================================
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAS Commander Toolbox Help</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f4f4f4;
        }
        .header {
            background-color: #2c3e50;
            color: white;
            padding: 20px;
            border-radius: 5px;
            margin-bottom: 30px;
        }
        .header h1 {
            margin: 0;
            font-size: 2.5em;
        }
        .header p {
            margin: 10px 0 0 0;
            font-size: 1.2em;
        }
        .tool-section {
            background-color: white;
            padding: 20px;
            margin-bottom: 20px;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .tool-section h2 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        .tool-section h3 {
            color: #34495e;
            margin-top: 20px;
        }
        .parameter {
            background-color: #ecf0f1;
            padding: 15px;
            margin: 10px 0;
            border-left: 4px solid #3498db;
            border-radius: 3px;
        }
        .parameter h4 {
            margin: 0 0 10px 0;
            color: #2c3e50;
        }
        .warning {
            background-color: #fff3cd;
            border: 1px solid #ffeaa7;
            border-left: 4px solid #f39c12;
            padding: 15px;
            margin: 20px 0;
            border-radius: 3px;
        }
        .warning h4 {
            margin: 0 0 10px 0;
            color: #856404;
        }
        .note {
            background-color: #d1ecf1;
            border: 1px solid #bee5eb;
            border-left: 4px solid #17a2b8;
            padding: 15px;
            margin: 20px 0;
            border-radius: 3px;
        }
        ul {
            margin: 10px 0;
            padding-left: 30px;
        }
        code {
            background-color: #f8f9fa;
            padding: 2px 5px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        .footer {
            text-align: center;
            margin-top: 40px;
            padding: 20px;
            background-color: #ecf0f1;
            border-radius: 5px;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>RAS Commander Toolbox</h1>
        <p>ArcGIS Python Toolbox for HEC-RAS HDF5 Data Integration</p>
    </div>

    <div class="tool-section">
        <h2>Overview</h2>
        <p>The RAS Commander Toolbox provides tools for loading and visualizing HEC-RAS 2D geometry, terrain, and results data from HDF5 files directly within ArcGIS Pro. This toolbox is a direct port of the HDF5 data extraction logic from the <a href="https://github.com/gpt-cmdr/ras-commander">ras-commander library</a>.</p>
        
        <h3>Sponsorship</h3>
        <p>The development and porting of these tools to the ArcGIS platform was generously sponsored by <a href="https://clbengineering.com/">CLB Engineering</a> in cooperation with ESRI.</p>
        
        <h3>Requirements</h3>
        <ul>
            <li>ArcGIS Pro 2.8 or higher</li>
            <li>HEC-RAS 6.x model files</li>
            <li>Python packages: h5py, numpy (included with ArcGIS Pro)</li>
        </ul>
    </div>

    <div class="tool-section">
        <h2>Load HEC-RAS 2D Geometry Layers</h2>
        <p>This tool extracts 2D geometry elements from HEC-RAS geometry (<code>g*.hdf</code>) or plan (<code>p*.hdf</code>) files.</p>
        
        <h3>Available Geometry Elements</h3>
        <ul>
            <li><strong>2D Breaklines</strong> - Mesh refinement lines with cell spacing attributes</li>
            <li><strong>2D Boundary Condition Lines</strong> - External and internal boundary conditions</li>
            <li><strong>Mesh Area Perimeters</strong> - 2D flow area boundaries</li>
            <li><strong>Mesh Cell Centers</strong> - Point locations at the center of each mesh cell</li>
            <li><strong>Mesh Cell Faces</strong> - Line geometries representing cell edges</li>
            <li><strong>Mesh Cells (Polygons)</strong> - Full polygon representation of mesh cells</li>
            <li><strong>Pipe Conduits</strong> - Storm/sewer pipe networks (if present)</li>
            <li><strong>Pipe Nodes</strong> - Junction points in pipe networks (if present)</li>
        </ul>
        
        <div class="note">
            <h4>Performance Note</h4>
            <p>Mesh cell polygon creation can be time-consuming for large meshes (>100,000 cells). Consider extracting only the necessary elements for your analysis.</p>
        </div>
        
        <h3>Parameters</h3>
        <div class="parameter">
            <h4>Geometry or Plan HDF File</h4>
            <p>Select a HEC-RAS geometry file (<code>g*.hdf</code>) or plan file (<code>p*.hdf</code>) containing 2D geometry data.</p>
        </div>
        <div class="parameter">
            <h4>Override CRS (Optional)</h4>
            <p>Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files. The tool will first attempt to read the CRS from the HDF file or associated .prj files.</p>
        </div>
        <div class="parameter">
            <h4>Geometry Elements to Load</h4>
            <p>Select one or more geometry elements to extract. Each selected element will create a separate output feature class.</p>
        </div>
    </div>

    <div class="tool-section">
        <h2>Load HEC-RAS 2D Results Summary Layers</h2>
        <p>This tool extracts summary results data from HEC-RAS plan files (<code>p*.hdf</code>) that contain simulation results.</p>
        
        <h3>Available Results</h3>
        <ul>
            <li><strong>Max WSE at Cell Centers</strong> - Maximum water surface elevation achieved at each cell center during the simulation, with the time of occurrence</li>
            <li><strong>Max Vel at Cell Faces</strong> - Maximum velocity achieved at each cell face during the simulation, with the time of occurrence</li>
        </ul>
        
        <div class="warning">
            <h4>Important</h4>
            <p>This tool requires a plan HDF file that contains results data. Geometry-only files will not work.</p>
        </div>
        
        <h3>Parameters</h3>
        <div class="parameter">
            <h4>Plan HDF File with Results</h4>
            <p>Select a HEC-RAS plan file (<code>p*.hdf</code>) that contains simulation results.</p>
        </div>
        <div class="parameter">
            <h4>Override CRS (Optional)</h4>
            <p>Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files.</p>
        </div>
        <div class="parameter">
            <h4>Results to Load</h4>
            <p>Select one or more results types to extract. Each selected result will create a separate output feature class.</p>
        </div>
    </div>

    <div class="tool-section">
        <h2>Load HEC-RAS Terrain</h2>
        <p>This tool reads terrain layer definitions from a HEC-RAS project's <code>.rasmap</code> file and loads the associated VRT (Virtual Raster) files into ArcGIS Pro.</p>
        
        <div class="warning">
            <h4>Important Limitations</h4>
            <ul>
                <li>This tool loads the underlying terrain TIFFs as a VRT with the priority from HEC-RAS in place</li>
                <li>It does <strong>NOT</strong> include terrain modifications done as vector terrain modifications in RAS Mapper</li>
                <li>Only the base terrain raster data will be loaded</li>
                <li>If your HEC-RAS model includes vector terrain modifications (breaklines, high ground, etc.), those modifications will not be reflected in the loaded terrain</li>
            </ul>
        </div>
        
        <h3>How It Works</h3>
        <ol>
            <li>Reads the <code>.rasmap</code> file associated with your HEC-RAS project</li>
            <li>Finds all terrain layers defined in the project</li>
            <li>Loads the corresponding VRT files that reference the underlying TIFF files</li>
            <li>Applies the layer priority as defined in HEC-RAS</li>
        </ol>
        
        <h3>Parameters</h3>
        <div class="parameter">
            <h4>HEC-RAS Project File (*.prj)</h4>
            <p>Select the HEC-RAS project file. The tool will automatically find the associated <code>.rasmap</code> file in the same directory.</p>
        </div>
        <div class="parameter">
            <h4>Import All Terrains</h4>
            <p>Check this box to import all terrain layers found in the project. When checked, the terrain selection list will be disabled.</p>
        </div>
        <div class="parameter">
            <h4>Terrains to Load</h4>
            <p>Select specific terrain layers to load. This list is populated from the terrains found in the <code>.rasmap</code> file.</p>
        </div>
    </div>

    <div class="tool-section">
        <h2>Common Issues and Solutions</h2>
        
        <h3>CRS Not Found</h3>
        <p>If the tool cannot determine the coordinate reference system:</p>
        <ul>
            <li>Check if a <code>.prj</code> file exists in your HEC-RAS project directory</li>
            <li>Use the "Override CRS" parameter to manually specify the correct projection</li>
        </ul>
        
        <h3>Field Name Conflicts</h3>
        <p>The tool automatically handles field name conflicts with ArcGIS system fields by:</p>
        <ul>
            <li>Renaming "Shape" to "Shape_Type"</li>
            <li>Appending "_USER" to other conflicting field names</li>
            <li>Replacing special characters with underscores</li>
        </ul>
        
        <h3>Large Dataset Performance</h3>
        <p>For models with large meshes:</p>
        <ul>
            <li>Consider extracting elements to file geodatabases instead of in-memory workspaces</li>
            <li>Extract only the necessary elements for your analysis</li>
            <li>Process mesh cells in smaller 2D flow areas if possible</li>
        </ul>
    </div>

    <div class="footer">
        <p><strong>RAS Commander Toolbox</strong> | Sponsored by <a href="https://clbengineering.com/">CLB Engineering</a></p>
        <p>Based on the <a href="https://github.com/gpt-cmdr/ras-commander">ras-commander library</a></p>
    </div>
</body>
</html>
==================================================

File: c:\GH\ras-commander-hydro\README_Help.md
==================================================
# RAS Commander Help Documentation

This directory contains help documentation for the RAS Commander Toolbox.

## Files

- **RASCommander_Help.html** - Main help documentation file that displays when users click the help button in ArcGIS Pro

## Help Integration

The help system is integrated into each tool through the `getHelp()` method in each tool class. When users click the help button (?) in the tool dialog:

1. ArcGIS Pro calls the tool's `getHelp()` method
2. The method returns a file:/// URL pointing to the HTML help file
3. The URL includes an anchor (#) to jump to the specific tool's section

## Updating Help

To update the help documentation:

1. Edit `RASCommander_Help.html` 
2. Make sure to maintain the anchor IDs for each section:
   - `#load-hec-ras-2d-geometry-layers`
   - `#load-hec-ras-2d-results-summary-layers`
   - `#load-hec-ras-terrain`

## VRT Limitations Note

The Load HEC-RAS Terrain tool includes important warnings about VRT limitations:
- The tool only loads base terrain VRT files
- Vector terrain modifications from RAS Mapper are NOT included
- This limitation is documented in:
  - The tool's class docstring
  - The tool's description
  - Warning messages during execution
  - The help documentation

## Styling

The HTML help file uses inline CSS for portability. The styling includes:
- Responsive design that works in various browser windows
- Color-coded sections for warnings and notes
- Clear parameter descriptions
- Professional appearance matching ArcGIS Pro's style
==================================================

File: c:\GH\ras-commander-hydro\TRADEMARKS.md
==================================================
# TRADEMARKS.md

## Project Trademark

“RAS Commander”™ is an unregistered trademark of CLB Engineering Corporation. The mark is used solely to identify this open‑source software project and related materials.

## Third‑Party Trademarks

* **HEC‑RAS**™ is a trademark of the U.S. Army Corps of Engineers (USACE) Hydrologic Engineering Center (HEC).
* **ARC HYDRO** is a trademark of Environmental Systems Research Institute (ESRI)

* "RAS Commander" and "RAS Commander Arc Hydro Tools" are independent open source projects and are **not** affiliated with, endorsed by, or sponsored by USACE or HEC.

All other product names, logos, and brands mentioned in this repository are property of their respective owners and are used for identification purposes only.

## Naming & Compliance Policy

We respect the trademark rights and license terms of USACE and all other rights holders. If USACE, ESRI—or any other rightful owner—objects to our use of “RAS” or "Arc Hydro" in the project name, we will promptly rename the project and update all references within **30 days** of receiving written notice.

## Contact

To raise any trademark or licensing concerns, please open an issue in this repository or e‑mail **[info@engineeringwithllms.info](mailto:info@engineeringwithllms.info)**.
 
==================================================

File: c:\GH\ras-commander-hydro\uninstall_toolbox.ps1
==================================================
# uninstall_toolbox.ps1
#
# Uninstalls the Arc Hydro RAS Commander toolbox from ArcGIS Pro.
# This script removes all toolbox files from the ArcGIS Pro installation directories.
#
# Usage:
#   Right-click on this file and select "Run with PowerShell" 
#   OR
#   Open PowerShell as Administrator and run: .\uninstall_toolbox.ps1
#
# Note: This script requires administrator privileges to remove files from Program Files.

# Check if running as administrator
if (-NOT ([Security.Principal.WindowsPrincipal] [Security.Principal.WindowsIdentity]::GetCurrent()).IsInRole([Security.Principal.WindowsBuiltInRole] "Administrator")) {
    Write-Host "This script requires Administrator privileges." -ForegroundColor Red
    Write-Host "Please run PowerShell as Administrator and try again." -ForegroundColor Yellow
    Write-Host ""
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 1
}

Write-Host "Arc Hydro RAS Commander Toolbox Uninstaller" -ForegroundColor Cyan
Write-Host ("=" * 50) -ForegroundColor Cyan
Write-Host ""

# Function to find ArcGIS Pro installation
function Find-ArcGISPro {
    $potentialPaths = @(
        "C:\Program Files\ArcGIS\Pro",
        "C:\Program Files (x86)\ArcGIS\Pro",
        "$env:ProgramFiles\ArcGIS\Pro",
        "${env:ProgramFiles(x86)}\ArcGIS\Pro"
    )
    
    # Check each potential path
    foreach ($path in $potentialPaths) {
        if (Test-Path $path) {
            $toolboxPath = Join-Path $path "Resources\ArcToolBox"
            if (Test-Path $toolboxPath) {
                return $path
            }
        }
    }
    
    # Check registry
    try {
        $regPath = "HKLM:\SOFTWARE\ESRI\ArcGISPro"
        if (Test-Path $regPath) {
            $installDir = (Get-ItemProperty -Path $regPath -Name InstallDir -ErrorAction SilentlyContinue).InstallDir
            if ($installDir -and (Test-Path $installDir)) {
                return $installDir
            }
        }
    } catch {
        # Registry check failed, continue
    }
    
    return $null
}

# Find ArcGIS Pro installation
$arcgisProPath = Find-ArcGISPro
if (-not $arcgisProPath) {
    Write-Host "ERROR: Could not find ArcGIS Pro installation." -ForegroundColor Red
    Write-Host "ArcGIS Pro may not be installed or the toolbox may already be uninstalled." -ForegroundColor Yellow
    Write-Host ""
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 1
}

Write-Host "Found ArcGIS Pro at: $arcgisProPath" -ForegroundColor Green
Write-Host ""

# Define paths to remove
$pathsToRemove = @(
    @{
        Name = "Python Scripts"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\Scripts\archydro\rc_*.py"
        Type = "Files"
    },
    @{
        Name = "Python Toolbox"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\toolboxes\RAS Commander.pyt"
        Type = "File"
    },
    @{
        Name = "Python Toolbox XML"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\toolboxes\RAS Commander.pyt.xml"
        Type = "File"
    },
    @{
        Name = "Layer Templates"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\Templates\Layers\archydro"
        Type = "Directory"
    },
    @{
        Name = "Geodatabase Template"
        Path = Join-Path $arcgisProPath "Resources\ArcToolBox\Data\archydro\Ras2DTemplate.gdb"
        Type = "Directory"
    }
)

# Note: We don't remove Images directory as it might contain files from other tools

Write-Host "This will remove the following Arc Hydro RAS Commander components:" -ForegroundColor Yellow
foreach ($item in $pathsToRemove) {
    if (Test-Path $item.Path) {
        # Check if it's a symlink
        $itemInfo = Get-Item $item.Path -Force -ErrorAction SilentlyContinue
        if ($itemInfo -and $itemInfo.LinkType) {
            Write-Host "  - $($item.Name) (symlink): $($item.Path)" -ForegroundColor Cyan
        } else {
            Write-Host "  - $($item.Name): $($item.Path)" -ForegroundColor White
        }
    }
}

Write-Host ""
$response = Read-Host "Do you want to proceed with uninstallation? (y/N)"

if ($response -ne 'y' -and $response -ne 'Y') {
    Write-Host ""
    Write-Host "Uninstallation cancelled." -ForegroundColor Yellow
    Write-Host "Press any key to exit..."
    $null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
    exit 0
}

Write-Host ""
Write-Host "Uninstalling Arc Hydro RAS Commander components..." -ForegroundColor Yellow
Write-Host ""

$successCount = 0
$errorCount = 0
$notFoundCount = 0

foreach ($item in $pathsToRemove) {
    Write-Host "Removing: $($item.Name)" -ForegroundColor White
    
    if (-not (Test-Path $item.Path)) {
        Write-Host "  Not found - skipping" -ForegroundColor Gray
        $notFoundCount++
        continue
    }
    
    try {
        # Check if it's a symlink
        $itemInfo = Get-Item $item.Path -Force -ErrorAction SilentlyContinue
        $isSymlink = $itemInfo -and $itemInfo.LinkType
        
        if ($item.Type -eq "Directory") {
            if ($isSymlink) {
                # For symlinked directories, just remove the link
                (Get-Item $item.Path).Delete()
                Write-Host "  Success: Removed symlink" -ForegroundColor Green
            } else {
                # For regular directories, remove recursively
                Remove-Item -Path $item.Path -Recurse -Force
                Write-Host "  Success: Removed directory and all contents" -ForegroundColor Green
            }
        } elseif ($item.Type -eq "Files") {
            # For file patterns (like rc_*.py)
            $parentPath = Split-Path $item.Path -Parent
            $pattern = Split-Path $item.Path -Leaf
            Get-ChildItem -Path $parentPath -Filter $pattern -ErrorAction SilentlyContinue | ForEach-Object {
                Remove-Item -Path $_.FullName -Force
            }
            Write-Host "  Success: Removed matching files" -ForegroundColor Green
        } else {
            # For single files (including symlinked files)
            Remove-Item -Path $item.Path -Force
            Write-Host "  Success: Removed file" -ForegroundColor Green
        }
        
        $successCount++
        
    } catch {
        Write-Host "  ERROR: Failed to remove - $_" -ForegroundColor Red
        $errorCount++
    }
}

Write-Host ""
Write-Host ("=" * 50) -ForegroundColor Cyan
Write-Host "Uninstallation Summary:" -ForegroundColor Cyan
Write-Host "  Removed: $successCount components" -ForegroundColor Green
Write-Host "  Failed: $errorCount components" -ForegroundColor $(if ($errorCount -gt 0) { "Red" } else { "Gray" })
Write-Host "  Not found: $notFoundCount components" -ForegroundColor Gray
Write-Host ""

if ($errorCount -gt 0) {
    Write-Host "Some components failed to uninstall. Please check the errors above." -ForegroundColor Yellow
    Write-Host "You may need to manually remove these items." -ForegroundColor Yellow
} elseif ($successCount -eq 0 -and $notFoundCount -gt 0) {
    Write-Host "No components were found to uninstall." -ForegroundColor Yellow
    Write-Host "The toolbox may have already been uninstalled." -ForegroundColor Yellow
} else {
    Write-Host "Arc Hydro RAS Commander has been successfully uninstalled!" -ForegroundColor Green
}

# Clean up empty parent directories if they exist
Write-Host ""
Write-Host "Cleaning up empty directories..." -ForegroundColor Yellow

$parentDirsToCheck = @(
    Join-Path $arcgisProPath "Resources\ArcToolBox\Scripts\ras_commander"
    Join-Path $arcgisProPath "Resources\ArcToolBox\Templates\Layers\archydro"
    Join-Path $arcgisProPath "Resources\ArcToolBox\Data\archydro"
)

foreach ($dir in $parentDirsToCheck) {
    $parent = Split-Path $dir -Parent
    if ((Test-Path $parent) -and (Get-ChildItem $parent -Force | Measure-Object).Count -eq 0) {
        try {
            Remove-Item $parent -Force
            Write-Host "  Removed empty directory: $parent" -ForegroundColor Green
        } catch {
            # Ignore errors for cleanup
        }
    }
}

Write-Host ""
Write-Host "Press any key to exit..."
$null = $Host.UI.RawUI.ReadKey("NoEcho,IncludeKeyDown")
==================================================

File: c:\GH\ras-commander-hydro\Resources\Customize tool behavior in a Python toolbox—ArcGIS Pro - Documentation.url
==================================================
[InternetShortcut]
URL=https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/customizing-tool-behavior-in-a-python-toolbox.htm

==================================================

File: c:\GH\ras-commander-hydro\Resources\ESRI Developer Links.txt
==================================================
https://developers.arcgis.com/



https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/customizing-tool-behavior-in-a-python-toolbox.htm



https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/updating-schema-in-a-python-toolbox.htm


https://pro.arcgis.com/en/pro-app/latest/arcpy/classes/schema.htm


https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/accessing-parameters-within-a-python-toolbox.htm


https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/accessing-parameters-within-a-python-toolbox.htm


https://pro.arcgis.com/en/pro-app/latest/arcpy/geoprocessing_and_python/writing-messages-in-a-python-toolbox.htm
==================================================

File: c:\GH\ras-commander-hydro\Resources\Esri Developer.url
==================================================
[InternetShortcut]
URL=https://developers.arcgis.com/

==================================================

File: c:\GH\ras-commander-hydro\testdata\Test Data Notes.txt
==================================================
The toolbox is only meant to support 6.x Model Series Results

1D Unsteady Model Default HDF For Unit Tests: BaldEagle.p01.hdf

1D Steady Model Default HDF For Unit Tests: BaldEagle.p02.hdf

2D Unsteady Default HDF For Unit Tests BaldEagleDamBrk.p07.hdf

2D Unsteady with Pipes and Pumps Default HDF For Unit Tests: DavisStormSystem.p02.hdf
==================================================

File: c:\GH\ras-commander-hydro\toolboxes\RAS Commander.LoadHECRAS1DGeometry.pyt.xml
==================================================
<?xml version="1.0" encoding="UTF-8"?>
<metadata xml:lang="en"><Esri><CreaDate>20250630</CreaDate><CreaTime>11091200</CreaTime><ArcGISFormat>1.0</ArcGISFormat><SyncOnce>TRUE</SyncOnce></Esri></metadata>

==================================================

File: c:\GH\ras-commander-hydro\toolboxes\RAS Commander.LoadHECRAS2DGeometry.pyt.xml
==================================================
<?xml version="1.0" encoding="UTF-8"?>
<metadata xml:lang="en"><Esri><CreaDate>20250630</CreaDate><CreaTime>11095000</CreaTime><ArcGISFormat>1.0</ArcGISFormat><SyncOnce>TRUE</SyncOnce></Esri></metadata>

==================================================

File: c:\GH\ras-commander-hydro\toolboxes\RAS Commander.LoadHECRAS2DResults.pyt.xml
==================================================
<?xml version="1.0" encoding="UTF-8"?>
<metadata xml:lang="en"><Esri><CreaDate>20250703</CreaDate><CreaTime>17041800</CreaTime><ArcGISFormat>1.0</ArcGISFormat><SyncOnce>TRUE</SyncOnce></Esri></metadata>

==================================================

File: c:\GH\ras-commander-hydro\toolboxes\RAS Commander.LoadRASTerrain.pyt.xml
==================================================
<?xml version="1.0" encoding="UTF-8"?>
<metadata xml:lang="en"><Esri><CreaDate>20250704</CreaDate><CreaTime>12280000</CreaTime><ArcGISFormat>1.0</ArcGISFormat><SyncOnce>TRUE</SyncOnce></Esri></metadata>

==================================================

File: c:\GH\ras-commander-hydro\toolboxes\RAS Commander.OrganizeRASProject.pyt.xml
==================================================
<?xml version="1.0" encoding="UTF-8"?>
<metadata xml:lang="en"><Esri><CreaDate>20250703</CreaDate><CreaTime>15590100</CreaTime><ArcGISFormat>1.0</ArcGISFormat><SyncOnce>TRUE</SyncOnce></Esri></metadata>

==================================================

File: c:\GH\ras-commander-hydro\toolboxes\RAS Commander.pyt
==================================================
# -*- coding: utf-8 -*-
#
# RAS Commander.pyt
#
# ArcGIS Python Toolbox for HEC-RAS HDF5 Data Integration
# ===================================================================================
#
# DESCRIPTION:
# This toolbox provides tools for loading and visualizing HEC-RAS 1D and 2D geometry,
# terrain, and results data from HDF5 files directly within ArcGIS Pro.
#
# ORIGIN AND ATTRIBUTION:
# This toolbox is a direct port of the HDF5 data extraction logic from the
# ras-commander library. All core HDF5 reading logic is derived from the
# library's HDF handling classes (e.g., HdfMesh, HdfBndry).
#
#   ras-commander library: https://github.com/gpt-cmdr/ras-commander
#
# SPONSORSHIP:
# The development and porting of these tools to the ArcGIS platform
# was generously sponsored by CLB Engineering in cooperation with ESRI.
#
#   CLB Engineering: https://clbengineering.com/
#
# ===================================================================================

import sys
import os

# Add the Scripts directory to the Python path so we can import our modules
toolbox_dir = os.path.dirname(os.path.abspath(__file__))
scripts_dir = os.path.join(os.path.dirname(toolbox_dir), 'Scripts', 'archydro')
if scripts_dir not in sys.path:
    sys.path.insert(0, scripts_dir)

# Import the tool classes from our modules
from rc_load_ras_terrain import LoadRASTerrain
from rc_load_hecras_2d_geometry import LoadHECRAS2DGeometry
from rc_load_hecras_2d_results import LoadHECRAS2DResults
from rc_load_hecras_1d_geometry import LoadHECRAS1DGeometry
from rc_organize_ras_project import OrganizeRASProject


class Toolbox(object):
    """
    ArcGIS Python Toolbox for loading HEC-RAS 1D and 2D geometry, terrain, and results layers.
    """
    def __init__(self):
        self.label = "Arc Hydro RAS-Commander Tools"
        self.alias = "RASCommander"
        self.description = "Tools for loading HEC-RAS 1D and 2D geometry, terrain, and results from HDF5 files. Sponsored by CLB Engineering (https://clbengineering.com/)."
        # List the tool classes
        self.tools = [LoadHECRAS1DGeometry, LoadHECRAS2DGeometry, LoadHECRAS2DResults, LoadRASTerrain, OrganizeRASProject]
==================================================

File: c:\GH\ras-commander-hydro\toolboxes\RAS Commander.pyt.xml
==================================================
<?xml version="1.0" encoding="UTF-8"?>
<metadata xml:lang="en"><Esri><CreaDate>20250630</CreaDate><CreaTime>11090300</CreaTime><ArcGISFormat>1.0</ArcGISFormat><SyncOnce>TRUE</SyncOnce><ModDate>20250704</ModDate><ModTime>093233</ModTime></Esri><toolbox name="RAS Commander" alias="RASCommander"><arcToolboxHelpPath>c:\program files\arcgis\pro\Resources\Help\gp</arcToolboxHelpPath><toolsets/></toolbox><dataIdInfo><idCitation><resTitle>RAS Commander</resTitle></idCitation></dataIdInfo><distInfo><distributor><distorFormat><formatName>ArcToolbox Toolbox</formatName></distorFormat></distributor></distInfo></metadata>

==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\New Naming Notes.txt
==================================================
To assist with packaging of this toolbox with Arc Hydro Tools, the scripts will now be located in /Scripts/archydro

All .py scripts will be prefixed by "rc_" (for ras commander)

Due to the fact that __init__.py already exists in the Arc Hydro folder, we will need to manage our toolbox without using __init__.py, and be aware that in production a different __init__.py exists that we cannot control.  

The Arc Hydro Team prefers that the RAS Commander Arc Hydro Tools conform to the existing flat file structure under /Scripts/archydro.  No other files exist in this folder with an rc_ prefix.  use lower case file names 

Installation and Uninstallation scripts need to be changed

Toolbox imports need to be revised

Script imports using init py need to be revised and eliminate use of the init py.  



Folder Mapping for ArcHydro Production: 

/Images/ras-commander-archydro-revised.png
This is used in the Help file and documentation

/Scripts/archydro 
All supporting .py files, organized to fit into the existing flat archydro file structure.  All .py files should start with rc_ prefix, use lower case file names and should not use __init__.py, as one already exists in this folder.  All imports should be relative, as the repository mirrors the final production folder structure.  

/Templates/Layers/archydro
Any layer lyrx files in this folder should be named lower case, with an "rc_" prefix.  I need a full guide on how to incorporate the use the .lyrx files into my toolbox's operations.  I understand I may need to calculate the max/min of a dataset to set the color ramp limits before inserting.  I need to figure out how to do this by looking up the ArcGIS Toolbox documentation and confirming how it is done elsewhere.  

.











We also need to figure out how to reliably document the toolbox within the ArcGIS toolbox environment (I have only had limited success).  There should be a full help file with the equivalent of a detailed README

I need more prominent branding for CLB Engineering and RAS Commander within the toolbox using the SVG

I need to add links to CLBEngineering.com and https://github.com/gpt-cmdr/ras-commander

Also the repository for this toolbox: https://github.com/gpt-cmdr/ras-commander-hydro

Write a Style Guide.md that includes the class and function naming conventions, and folder mapping and Naming Notes to keep the toolbox compatible with the existing build process for Arc Hydro Tools.   The Arc Hydro Developers will be cloning the repository 



Write a README.md for the repository


I also need a testing script that runs each tool in the toolbox over BaldEagle.p01.hdf (1D), BaldEagleDamBrk.p07.hdf (2D) and BeaverLakeSWMMImpor.p01.hdf (2D Pipes and Conduits) to build a full test suite.  




C:\Program Files\ArcGIS\Pro\Resources\ArcToolBox  is the location of ArcGIS's ArcToolbox folder on the local development machine.  Examples of how existing toolboxes operate can be found here, although no .pyt toolboxes are currently included. Search the web where needed to confirm how to perform specific goals in pyt toolboxes












Initial Release

- 1D and 2D Geometry Imports - Including Pipes and Conduits!



Future Releases

- 1D Results

- Pipes and Conduits Results

- Full Time Series Results

- Land Use Layer Imports

- Generating 2D Data in ArcGIS
	- Sync Mesh Perimeter Changes back to HEC-RAS
	- Sync Breaklines back to HEC-RAS
	- 

- Detailed Runtime Information (Plan, Unsteady Data for Model Inventories)


- Sync Active Layers to/from RAS Mapper

- Floodplain Mapping




PRESENTATION



This is focused on the HEC-RAS 6.x series - Older Versions (1D only) are supported by existing Arc Hydro Tools

HEC-RAS 2025 is not yet ready for flood studies - support will be considered once development is stable



Initial Release

Initial release is focused on importing basic elements to *speed the creation of report figures and display of mesh results*

- 1D and 2D Geometry Imports - Including Pipes and Conduits!

- 2D Mesh Cells as Polygons 

- 2D Mesh Results 

- Organize an Entire HEC-RAS Project as Geodatabase


Future Releases

- 1D Results

- Pipes and Conduits Results

- Full Time Series Results

- Land Use Layer Imports

- Generating 2D Data in ArcGIS
	- Sync Mesh Perimeter Changes back to HEC-RAS
	- Sync Breaklines back to HEC-RAS
	- 

- Detailed Runtime Information (Plan, Unsteady Data for Model Inventories)


- Sync Active Layers to/from RAS Mapper

- Floodplain Mapping

In the presentation, we should say this is a "Community Driven Effort - So we are looking for your feedback

Are you a?  Municipality Looking to Include Rich HEC-RAS Data in your Dashboards?  And engineer communicating multi-hazard flood risk?  What preprocessing and postprocessing steps are you most interested in? Looking to use ArcGIS to prepare 2D model data?  Tell us what you would like to see: We are looking for your feedback to shape development efforts."

How can we help you use HEC-RAS data for your municipal dashboards?

Do you need to build Model Inventories for Local Flood Studies and BLE data?
























==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_load_hecras_1d_geometry.py
==================================================
# -*- coding: utf-8 -*-
"""
LoadHECRAS1DGeometry.py

Tool for loading HEC-RAS 1D geometry layers from HDF files including cross sections,
river centerlines, bank lines, and hydraulic structures.
"""

import arcpy
import os
import h5py
import numpy as np

# Import helper functions from utils
from rc_utils import (
    get_ras_projection_wkt,
    cache_hdf_metadata,
    write_features_to_fc,
    get_dynamic_fields_from_data,
    setup_geodatabase_output,
    get_unique_fc_name,
    add_feature_class_metadata,
    extract_project_and_plan_info,
    create_geodatabase_from_hdf,
    get_feature_dataset_name,
    get_feature_class_name
)


class LoadHECRAS1DGeometry(object):
    """
    Loads 1D geometry elements from a HEC-RAS HDF file.
    """
    def __init__(self):
        self.label = "Load HEC-RAS 1D Geometry Layers"
        self.description = """Extracts 1D geometry elements from a HEC-RAS HDF file including cross sections, river centerlines, bank lines, and hydraulic structures.
        
        This tool extracts various 1D geometry elements from HEC-RAS geometry (g*.hdf) or plan (p*.hdf) files.
        
        Available geometry elements include:
        • Cross Sections - River cross section cut lines with station-elevation data
        • River Centerlines - Main river/reach centerlines
        • Bank Lines - Left and right bank lines
        • Edge Lines - River edge lines for terrain processing
        • 1D Structures - Bridges, culverts, weirs, and other structures
        
        Note: Each selected element will create a separate feature class."""
        self.canRunInBackground = False
        
        # Geometry elements
        self.CROSS_SECTIONS = "Cross Sections"
        self.RIVER_CENTERLINES = "River Centerlines"
        self.BANK_LINES = "Bank Lines"
        self.EDGE_LINES = "Edge Lines"
        self.STRUCTURES = "1D Structures"
        
        # Cache for HDF metadata
        self._hdf_cache = {}

    def getParameterInfo(self):
        geometry_elements = [self.CROSS_SECTIONS, self.RIVER_CENTERLINES, self.BANK_LINES, 
                           self.EDGE_LINES, self.STRUCTURES]

        params = [
            arcpy.Parameter(displayName="Geometry or Plan HDF File", name="input_hdf", datatype="DEFile", 
                          parameterType="Required", direction="Input"),
            arcpy.Parameter(displayName="Override CRS (Optional)", name="override_crs", datatype="GPSpatialReference", 
                          parameterType="Optional", direction="Input"),
            
            # Geometry elements to load
            arcpy.Parameter(displayName="Geometry Elements to Load", name="geometry_elements", datatype="GPString", 
                          parameterType="Required", direction="Input", multiValue=True),
            
            # Output parameters
            arcpy.Parameter(displayName="Output Cross Sections", name="output_cross_sections", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output River Centerlines", name="output_centerlines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Bank Lines", name="output_bank_lines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Edge Lines", name="output_edge_lines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output 1D Structures", name="output_structures", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            
            # Geodatabase organization parameters
            arcpy.Parameter(displayName="Output Geodatabase (Optional)", name="output_gdb", datatype="DEWorkspace", 
                          parameterType="Optional", direction="Output", category="Output"),
            arcpy.Parameter(displayName="Create New Geodatabase", name="create_gdb", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input", category="Output")
        ]
        
        # Configure HDF file filter
        params[0].filter.list = ["hdf", "g*.hdf", "p*.hdf"]
        params[0].description = "Select a HEC-RAS geometry file (g*.hdf) or plan file (p*.hdf) containing 1D geometry data."
        
        params[1].description = """Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files. 
        The tool will first attempt to read the CRS from the HDF file or associated .prj files."""
        
        # Set filters for multi-value parameters
        params[2].filter.type = "ValueList"
        params[2].filter.list = geometry_elements
        params[2].value = [self.CROSS_SECTIONS, self.RIVER_CENTERLINES]  # Default selection
        params[2].description = """Select one or more geometry elements to extract from the HDF file. 
        Each selected element will create a separate output feature class."""
        
        # Set default output paths and descriptions
        params[3].value = r"memory\CrossSections"
        params[3].description = "Output feature class for 1D cross sections with attributes."
        
        params[4].value = r"memory\RiverCenterlines"
        params[4].description = "Output feature class for river/reach centerlines."
        
        params[5].value = r"memory\BankLines"
        params[5].description = "Output feature class for left and right bank lines."
        
        params[6].value = r"memory\EdgeLines"
        params[6].description = "Output feature class for river edge lines."
        
        params[7].value = r"memory\Structures1D"
        params[7].description = "Output feature class for 1D structures (bridges, culverts, etc.)."
        
        # Geodatabase parameters
        params[8].description = """Specify a geodatabase to organize all output feature classes. 
        If provided, outputs will be created in this geodatabase instead of the default locations."""
        
        params[9].value = True  # Default to creating new geodatabase
        params[9].description = """Create a new geodatabase based on the HDF file name. 
        The geodatabase will be named using the pattern: ProjectName.pXX.gdb"""
        
        return params

    def isLicensed(self):
        """Set whether tool is licensed to execute."""
        return True

    def updateParameters(self, parameters):
        """Modify the values and properties of parameters before internal validation."""
        # Enable/disable output parameters based on selected elements
        if parameters[2].value:
            selected = parameters[2].valueAsText.split(';') if parameters[2].valueAsText else []
            
            # Enable/disable outputs based on selection
            parameters[3].enabled = self.CROSS_SECTIONS in selected
            parameters[4].enabled = self.RIVER_CENTERLINES in selected
            parameters[5].enabled = self.BANK_LINES in selected
            parameters[6].enabled = self.EDGE_LINES in selected
            parameters[7].enabled = self.STRUCTURES in selected
            
        # Auto-populate geodatabase path when HDF file is selected
        if parameters[0].value and parameters[0].altered:  # input_hdf
            hdf_path = parameters[0].valueAsText
            
            # If create_gdb is True, auto-populate geodatabase path
            if parameters[9].value:  # create_gdb
                project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
                gdb_name = f"{base_name}.gdb"
                gdb_path = os.path.join(os.path.dirname(hdf_path), gdb_name)
                parameters[8].value = gdb_path
        return
    
    def updateMessages(self, parameters):
        """Modify messages created by internal validation."""
        # Clear geodatabase validation error if create_gdb is True
        if parameters[9].value and parameters[8].hasError():  # create_gdb and output_gdb has error
            parameters[8].clearMessage()
        return

    # --- HDF Data Extraction Methods ---

    def _get_cross_sections_direct(self, hdf_file, sr):
        """Extracts cross sections from HDF file."""
        try:
            xs_path = "Geometry/Cross Sections"
            if xs_path not in hdf_file:
                arcpy.AddMessage("No cross sections found in HDF file.")
                return [], []
            
            # Check if required datasets exist
            required_datasets = ["Attributes", "Polyline Info", "Polyline Points", 
                               "Station Elevation Info", "Station Elevation Values"]
            for dataset in required_datasets:
                if f"{xs_path}/{dataset}" not in hdf_file:
                    arcpy.AddWarning(f"Cross sections data incomplete: missing '{dataset}' dataset.")
                    return [], []
            
            # Get attributes
            attributes = hdf_file[f"{xs_path}/Attributes"][()]
            
            # Get polyline geometry
            polyline_info = hdf_file[f"{xs_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{xs_path}/Polyline Points"][()]
            
            # Get station-elevation data
            sta_elev_info = hdf_file[f"{xs_path}/Station Elevation Info"][()]
            sta_elev_values = hdf_file[f"{xs_path}/Station Elevation Values"][()]
            
            # Get Manning's n data
            mannings_info = hdf_file[f"{xs_path}/Manning's n Info"][()]
            mannings_values = hdf_file[f"{xs_path}/Manning's n Values"][()]
            
            valid_data, geometries = [], []
            
            for idx, attr in enumerate(attributes):
                # Get polyline info
                pnt_start, pnt_cnt, _, _ = polyline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                # Extract attributes
                river = attr["River"].decode('utf-8', 'ignore').strip()
                reach = attr["Reach"].decode('utf-8', 'ignore').strip()
                rs = attr["RS"].decode('utf-8', 'ignore').strip()
                
                # Get station-elevation profile
                se_start, se_count = sta_elev_info[idx]
                sta_elev_pairs = []
                if se_count > 0:
                    se_data = sta_elev_values[se_start:se_start + se_count]
                    sta_elev_pairs = [(float(s), float(e)) for s, e in se_data]
                
                # Get Manning's n profile
                mn_start, mn_count = mannings_info[idx]
                mannings_pairs = []
                if mn_count > 0:
                    mn_data = mannings_values[mn_start:mn_start + mn_count]
                    mannings_pairs = [(float(s), float(n)) for s, n in mn_data]
                
                valid_data.append({
                    'xs_id': int(idx),
                    'River': river,
                    'Reach': reach,
                    'RS': rs,
                    'LeftBank': float(attr["Left Bank"]),
                    'RightBank': float(attr["Right Bank"]),
                    'LenLeft': float(attr["Len Left"]),
                    'LenChannel': float(attr["Len Channel"]),
                    'LenRight': float(attr["Len Right"]),
                    'StationElevation': str(sta_elev_pairs)[:255],  # Convert to string for field storage
                    'ManningsN': str(mannings_pairs)[:255]
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Error reading cross sections: {e}")
            return [], []

    def _get_river_centerlines_direct(self, hdf_file, sr):
        """Extracts river centerlines from HDF file."""
        try:
            centerlines_path = "Geometry/River Centerlines"
            if centerlines_path not in hdf_file:
                return [], []
            
            # Get attributes
            attributes = hdf_file[f"{centerlines_path}/Attributes"][()]
            
            # Get polyline geometry
            polyline_info = hdf_file[f"{centerlines_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{centerlines_path}/Polyline Points"][()]
            
            valid_data, geometries = [], []
            
            for idx, attr in enumerate(attributes):
                # Get polyline info
                pnt_start, pnt_cnt, _, _ = polyline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                # Extract attributes
                river_name = attr["River Name"].decode('utf-8', 'ignore').strip()
                reach_name = attr["Reach Name"].decode('utf-8', 'ignore').strip()
                
                valid_data.append({
                    'river_id': int(idx),
                    'RiverName': river_name,
                    'ReachName': reach_name,
                    'USType': attr["US Type"].decode('utf-8', 'ignore').strip(),
                    'DSType': attr["DS Type"].decode('utf-8', 'ignore').strip()
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (River Centerlines): {e}")
            raise arcpy.ExecuteError("Failed to read river centerlines from HDF file")

    def _get_bank_lines_direct(self, hdf_file, sr):
        """Extracts bank lines from HDF file."""
        try:
            bank_lines_path = "Geometry/River Bank Lines"
            if bank_lines_path not in hdf_file:
                arcpy.AddMessage("No bank lines found in HDF file.")
                return [], []
            
            # Get polyline geometry
            polyline_info = hdf_file[f"{bank_lines_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{bank_lines_path}/Polyline Points"][()]
            
            valid_data, geometries = [], []
            
            # Bank lines typically come in pairs (left and right)
            bank_sides = ['Left', 'Right']
            
            for idx in range(len(polyline_info)):
                # Get polyline info
                pnt_start, pnt_cnt, _, _ = polyline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                # Determine bank side
                bank_side = bank_sides[idx % 2] if idx < len(bank_sides) else f"Bank_{idx}"
                
                valid_data.append({
                    'bank_id': int(idx),
                    'BankSide': bank_side,
                    'Length': float(geom.length)
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Error reading bank lines: {e}")
            return [], []

    def _get_edge_lines_direct(self, hdf_file, sr):
        """Extracts edge lines from HDF file."""
        try:
            edge_lines_path = "Geometry/River Edge Lines"
            if edge_lines_path not in hdf_file:
                arcpy.AddMessage("No edge lines found in HDF file.")
                return [], []
            
            # Get polyline geometry
            polyline_info = hdf_file[f"{edge_lines_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{edge_lines_path}/Polyline Points"][()]
            
            valid_data, geometries = [], []
            
            for idx in range(len(polyline_info)):
                # Get polyline info
                pnt_start, pnt_cnt, _, _ = polyline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                valid_data.append({
                    'edge_id': int(idx),
                    'EdgeType': f"Edge_{idx}",
                    'Length': float(geom.length)
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Error reading edge lines: {e}")
            return [], []

    def _get_structures_direct(self, hdf_file, sr):
        """Extracts hydraulic structures from HDF file."""
        try:
            structures_path = "Geometry/Structures"
            if structures_path not in hdf_file:
                arcpy.AddMessage("No hydraulic structures found in HDF file.")
                return [], []
            
            # Get attributes
            attributes = hdf_file[f"{structures_path}/Attributes"][()]
            
            # Get centerline geometry
            centerline_info = hdf_file[f"{structures_path}/Centerline Info"][()]
            centerline_points = hdf_file[f"{structures_path}/Centerline Points"][()]
            
            valid_data, geometries = [], []
            
            for idx, attr in enumerate(attributes):
                # Get centerline info
                pnt_start, pnt_cnt, _, _ = centerline_info[idx]
                
                if pnt_cnt < 2:
                    continue
                
                # Extract points and create polyline
                points = centerline_points[pnt_start:pnt_start + pnt_cnt]
                arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                geom = arcpy.Polyline(arcpy_array, sr)
                
                # Extract attributes
                struct_type = attr["Type"].decode('utf-8', 'ignore').strip()
                river = attr["River"].decode('utf-8', 'ignore').strip()
                reach = attr["Reach"].decode('utf-8', 'ignore').strip()
                rs = attr["RS"].decode('utf-8', 'ignore').strip()
                
                valid_data.append({
                    'struct_id': int(idx),
                    'Type': struct_type,
                    'River': river,
                    'Reach': reach,
                    'RS': rs,
                    'Description': attr["Description"].decode('utf-8', 'ignore').strip()[:255]
                })
                geometries.append(geom)
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Error reading 1D structures: {e}")
            return [], []

    # --- Main Execution Logic ---
    def execute(self, parameters, messages):
        hdf_path = parameters[0].valueAsText
        
        # Get selected elements
        geometry_elements = parameters[2].values if parameters[2].values else []
        
        if not geometry_elements:
            messages.addErrorMessage("No geometry elements selected for loading. Please select at least one element.")
            raise arcpy.ExecuteError
        
        # Get geodatabase parameters
        output_gdb = parameters[8].valueAsText
        create_gdb = parameters[9].value
        output_workspace = None
        
        # Extract project and plan info
        project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
        
        # Get projection
        proj_wkt = get_ras_projection_wkt(hdf_path)
        sr = None
        if proj_wkt:
            sr = arcpy.SpatialReference()
            sr.loadFromString(proj_wkt)
            messages.addMessage(f"CRS '{sr.name}' found in HEC-RAS project files.")
        elif parameters[1].value:
            sr = parameters[1].value
            messages.addMessage(f"Using user-defined override CRS: {sr.name}")
        else:
            messages.addErrorMessage("CRS could not be determined. Please use the Override CRS parameter.")
            raise arcpy.ExecuteError
        
        # Setup geodatabase
        if create_gdb or output_gdb:
            if create_gdb and not output_gdb:
                # Auto-create geodatabase based on HDF name
                output_gdb = create_geodatabase_from_hdf(hdf_path, messages)
            
            # Create feature dataset with project/plan naming
            feature_dataset_name = get_feature_dataset_name(hdf_path)
            output_workspace = setup_geodatabase_output(output_gdb, feature_dataset_name, sr, messages)
            messages.addMessage(f"Output workspace set to: {output_workspace}")
        
        # Open HDF file once
        with h5py.File(hdf_path, 'r') as hdf_file:
            messages.addMessage("Reading HDF file structure...")
            
            # Process geometry elements
            if self.CROSS_SECTIONS in geometry_elements and parameters[3].valueAsText:
                output_fc = parameters[3].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "CrossSections"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[3].value = output_fc
                
                messages.addMessage("Extracting Cross Sections...")
                data, geoms = self._get_cross_sections_direct(hdf_file, sr)
                fields = [("xs_id", "LONG"), ("River", "TEXT"), ("Reach", "TEXT"), 
                         ("RS", "TEXT"), ("LeftBank", "DOUBLE"), ("RightBank", "DOUBLE"),
                         ("LenLeft", "DOUBLE"), ("LenChannel", "DOUBLE"), ("LenRight", "DOUBLE"),
                         ("StationElevation", "TEXT", 255), ("ManningsN", "TEXT", 255)]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "River cross section cut lines with station-elevation data", hdf_path)
            
            if self.RIVER_CENTERLINES in geometry_elements and parameters[4].valueAsText:
                output_fc = parameters[4].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "RiverCenterlines"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[4].value = output_fc
                
                messages.addMessage("Extracting River Centerlines...")
                data, geoms = self._get_river_centerlines_direct(hdf_file, sr)
                fields = [("river_id", "LONG"), ("RiverName", "TEXT"), ("ReachName", "TEXT"),
                         ("USType", "TEXT"), ("DSType", "TEXT")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Main river/reach centerlines", hdf_path)
            
            if self.BANK_LINES in geometry_elements and parameters[5].valueAsText:
                output_fc = parameters[5].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "BankLines"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[5].value = output_fc
                
                messages.addMessage("Extracting Bank Lines...")
                data, geoms = self._get_bank_lines_direct(hdf_file, sr)
                fields = [("bank_id", "LONG"), ("BankSide", "TEXT"), ("Length", "DOUBLE")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Left and right bank lines", hdf_path)
            
            if self.EDGE_LINES in geometry_elements and parameters[6].valueAsText:
                output_fc = parameters[6].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "EdgeLines"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[6].value = output_fc
                
                messages.addMessage("Extracting Edge Lines...")
                data, geoms = self._get_edge_lines_direct(hdf_file, sr)
                fields = [("edge_id", "LONG"), ("EdgeType", "TEXT"), ("Length", "DOUBLE")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "River edge lines for terrain processing", hdf_path)
            
            if self.STRUCTURES in geometry_elements and parameters[7].valueAsText:
                output_fc = parameters[7].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "Structures1D"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[7].value = output_fc
                
                messages.addMessage("Extracting 1D Structures...")
                data, geoms = self._get_structures_direct(hdf_file, sr)
                fields = [("struct_id", "LONG"), ("Type", "TEXT"), ("River", "TEXT"),
                         ("Reach", "TEXT"), ("RS", "TEXT"), ("Description", "TEXT", 255)]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Bridges, culverts, weirs, and other 1D structures", hdf_path)
        
        messages.addMessage("\nProcessing complete.")
        return
    
    def getHelp(self, tool_name):
        """Return help documentation URL for the tool."""
        help_file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
                                "Doc", "RASCommander_Help.html")
        if os.path.exists(help_file):
            return f"file:///{help_file.replace(os.sep, '/')}#load-hec-ras-1d-geometry-layers"
        return None
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_load_hecras_2d_geometry.py
==================================================
# -*- coding: utf-8 -*-
"""
LoadHECRAS2DGeometry.py

Tool for loading HEC-RAS 2D geometry layers from HDF files including mesh elements,
breaklines, boundary conditions, and pipe networks.
"""

import arcpy
import os
import h5py
import numpy as np
from collections import defaultdict

# Import helper functions from utils
from rc_utils import (
    get_ras_projection_wkt,
    polygonize_arcpy_optimized,
    get_polyline_centroid_vectorized,
    cache_hdf_metadata,
    write_features_to_fc,
    get_dynamic_fields_from_data,
    setup_geodatabase_output,
    get_unique_fc_name,
    add_feature_class_metadata,
    extract_project_and_plan_info,
    create_geodatabase_from_hdf,
    get_feature_dataset_name,
    get_feature_class_name
)


class LoadHECRAS2DGeometry(object):
    """
    Loads 2D geometry elements from a HEC-RAS HDF file.
    """
    def __init__(self):
        self.label = "Load HEC-RAS 2D Geometry Layers"
        self.description = """Extracts 2D geometry elements from a HEC-RAS HDF file including mesh elements, breaklines, boundary conditions, and pipe networks.
        
        This tool extracts various 2D geometry elements from HEC-RAS geometry (g*.hdf) or plan (p*.hdf) files.
        
        Available geometry elements include:
        • 2D Breaklines - Mesh refinement lines with cell spacing attributes
        • 2D Boundary Condition Lines - External and internal boundary conditions
        • Mesh Area Perimeters - 2D flow area boundaries
        • Mesh Cell Centers - Point locations at the center of each mesh cell
        • Mesh Cell Faces - Line geometries representing cell edges
        • Mesh Cells (Polygons) - Full polygon representation of mesh cells
        • Pipe Conduits - Storm/sewer pipe networks (if present)
        • Pipe Nodes - Junction points in pipe networks (if present)
        
        Note: Mesh cell polygon creation can be time-consuming for large meshes (>100,000 cells)."""
        self.canRunInBackground = False
        
        # Geometry elements
        self.BREAKLINES = "2D Breaklines"
        self.BC_LINES = "2D Boundary Condition Lines"
        self.PERIMETERS = "Mesh Area Perimeters"
        self.CELL_POINTS = "Mesh Cell Centers"
        self.CELL_FACES = "Mesh Cell Faces"
        self.CELL_POLYS = "Mesh Cells (Polygons)"
        
        # Pipe network elements
        self.PIPE_CONDUITS = "Pipe Conduits"
        self.PIPE_NODES = "Pipe Nodes"
        self.PIPE_NETWORKS = "Pipe Networks"
        
        # Cache for HDF metadata
        self._hdf_cache = {}

    def getParameterInfo(self):
        geometry_elements = [self.BREAKLINES, self.BC_LINES, self.PERIMETERS, self.CELL_POINTS, 
                           self.CELL_FACES, self.CELL_POLYS, self.PIPE_CONDUITS, self.PIPE_NODES, self.PIPE_NETWORKS]

        params = [
            arcpy.Parameter(displayName="Geometry or Plan HDF File", name="input_hdf", datatype="DEFile", 
                          parameterType="Required", direction="Input"),
            arcpy.Parameter(displayName="Override CRS (Optional)", name="override_crs", datatype="GPSpatialReference", 
                          parameterType="Optional", direction="Input"),
            
            # Geometry elements to load
            arcpy.Parameter(displayName="Geometry Elements to Load", name="geometry_elements", datatype="GPString", 
                          parameterType="Required", direction="Input", multiValue=True),
            
            # Output parameters
            arcpy.Parameter(displayName="Output 2D Breaklines", name="output_breaklines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output 2D Boundary Condition Lines", name="output_bc_lines", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Mesh Area Perimeters", name="output_perimeters", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Mesh Cell Centers", name="output_cell_points", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Mesh Cell Faces", name="output_cell_faces", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Mesh Cells (Polygons)", name="output_cell_polys", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Pipe Conduits", name="output_pipe_conduits", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Pipe Nodes", name="output_pipe_nodes", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Pipe Networks", name="output_pipe_networks", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            
            # Geodatabase organization parameters
            arcpy.Parameter(displayName="Output Geodatabase (Optional)", name="output_gdb", datatype="DEWorkspace", 
                          parameterType="Optional", direction="Output", category="Output"),
            arcpy.Parameter(displayName="Create New Geodatabase", name="create_gdb", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input", category="Output")
        ]
        
        # Configure HDF file filter
        params[0].filter.list = ["hdf", "g*.hdf", "p*.hdf"]
        params[0].description = "Select a HEC-RAS geometry file (g*.hdf) or plan file (p*.hdf) containing 2D geometry data."
        
        params[1].description = """Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files. 
        The tool will first attempt to read the CRS from the HDF file or associated .prj files."""
        
        # Set filters for multi-value parameters
        params[2].filter.type = "ValueList"
        params[2].filter.list = geometry_elements
        params[2].value = [self.PERIMETERS]  # Default selection
        params[2].description = """Select one or more geometry elements to extract from the HDF file. 
        Each selected element will create a separate output feature class."""
        
        # Set default output paths and descriptions
        params[3].value = r"memory\Breaklines"
        params[3].description = "Output feature class for 2D breaklines with cell spacing attributes."
        
        params[4].value = r"memory\BoundaryConditionLines"
        params[4].description = "Output feature class for 2D boundary condition lines."
        
        params[5].value = r"memory\MeshPerimeters"
        params[5].description = "Output feature class for 2D flow area perimeter polygons."
        
        params[6].value = r"memory\MeshCellCenters"
        params[6].description = "Output feature class for mesh cell center points."
        
        params[7].value = r"memory\MeshCellFaces"
        params[7].description = "Output feature class for mesh cell face polylines."
        
        params[8].value = r"memory\MeshCellPolygons"
        params[8].description = "Output feature class for mesh cell polygons. Note: Can be slow for large meshes."
        
        params[9].value = r"memory\PipeConduits"
        params[9].description = "Output feature class for pipe conduits (storm/sewer networks)."
        
        params[10].value = r"memory\PipeNodes"
        params[10].description = "Output feature class for pipe junction nodes."
        
        params[11].value = r"memory\PipeNetworks"
        params[11].description = "Output feature class for pipe network elements."
        
        # Geodatabase parameters
        params[12].description = """Specify a geodatabase to organize all output feature classes. 
        If provided, outputs will be created in this geodatabase instead of the default locations."""
        
        params[13].value = True  # Default to creating new geodatabase
        params[13].description = """Create a new geodatabase based on the HDF file name. 
        The geodatabase will be named using the pattern: ProjectName.pXX.gdb"""
        
        return params

    def isLicensed(self):
        """Set whether tool is licensed to execute."""
        return True

    def updateParameters(self, parameters):
        """Modify the values and properties of parameters before internal validation."""
        # Enable/disable output parameters based on selected elements
        if parameters[2].value:
            selected = parameters[2].valueAsText.split(';') if parameters[2].valueAsText else []
            
            # Enable/disable outputs based on selection
            parameters[3].enabled = self.BREAKLINES in selected
            parameters[4].enabled = self.BC_LINES in selected
            parameters[5].enabled = self.PERIMETERS in selected
            parameters[6].enabled = self.CELL_POINTS in selected
            parameters[7].enabled = self.CELL_FACES in selected
            parameters[8].enabled = self.CELL_POLYS in selected
            parameters[9].enabled = self.PIPE_CONDUITS in selected
            parameters[10].enabled = self.PIPE_NODES in selected
            parameters[11].enabled = self.PIPE_NETWORKS in selected
            
        # Auto-populate geodatabase path when HDF file is selected
        if parameters[0].value and parameters[0].altered:  # input_hdf
            hdf_path = parameters[0].valueAsText
            
            # If create_gdb is True, auto-populate geodatabase path
            if parameters[13].value:  # create_gdb
                project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
                gdb_name = f"{base_name}.gdb"
                gdb_path = os.path.join(os.path.dirname(hdf_path), gdb_name)
                parameters[12].value = gdb_path
        return
    
    def updateMessages(self, parameters):
        """Modify messages created by internal validation."""
        # Add warning for large mesh polygon creation
        if parameters[2].value and self.CELL_POLYS in str(parameters[2].value):
            parameters[8].setWarningMessage(
                "Creating cell polygons can be time-consuming for large meshes (>100,000 cells). "
                "Consider using cell centers or faces for visualization instead."
            )
        
        # Clear geodatabase validation error if create_gdb is True
        if parameters[13].value and parameters[12].hasError():  # create_gdb and output_gdb has error
            parameters[12].clearMessage()
        
        return

    # --- HDF Data Extraction Methods ---

    def _get_breaklines_direct(self, hdf_file, sr):
        """Extracts 2D breaklines from HDF file with optimized numpy operations."""
        try:
            breaklines_path = "Geometry/2D Flow Area Break Lines"
            if breaklines_path not in hdf_file:
                return [], []
            
            bl_line_data = hdf_file[breaklines_path]
            attributes = bl_line_data["Attributes"][()]
            polyline_info = bl_line_data["Polyline Info"][()]
            polyline_points = bl_line_data["Polyline Points"][()]
            
            # Vectorized filtering of valid breaklines
            valid_mask = polyline_info[:, 1] >= 2  # pnt_cnt >= 2
            valid_indices = np.where(valid_mask)[0]
            
            valid_data, geometries = [], []
            
            for idx in valid_indices:
                pnt_start, pnt_cnt, part_start, part_cnt = polyline_info[idx]
                attr_row = attributes[idx]
                
                name = attr_row["Name"]
                name = name.decode('utf-8', 'ignore').strip() if isinstance(name, bytes) else str(name)
                
                try:
                    # Extract points efficiently
                    points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                    
                    if part_cnt == 1:
                        # Single part - direct creation
                        arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                        geom = arcpy.Polyline(arcpy_array, sr)
                    else:
                        # Multi-part polyline
                        parts = bl_line_data["Polyline Parts"][()][part_start:part_start + part_cnt]
                        all_parts_array = arcpy.Array()
                        
                        for part_pnt_start, part_pnt_cnt in parts:
                            if part_pnt_cnt > 1:
                                part_points = points[part_pnt_start:part_pnt_start + part_pnt_cnt]
                                part_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in part_points])
                                all_parts_array.add(part_array)
                        
                        if all_parts_array.count == 0:
                            continue
                        geom = arcpy.Polyline(all_parts_array, sr)
                    
                    valid_data.append({
                        'bl_id': int(idx),
                        'Name': name,
                        'CellSpaceNear': float(attr_row["Cell Spacing Near"]),
                        'CellSpaceFar': float(attr_row["Cell Spacing Far"]),
                        'NearRepeats': int(attr_row["Near Repeats"]),
                        'ProtectRadius': int(attr_row["Protection Radius"])
                    })
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing breakline {idx}: {str(e)}")
                    continue
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Breaklines): {e}")
            raise arcpy.ExecuteError("Failed to read breaklines from HDF file")

    def _get_bc_lines_direct(self, hdf_file, sr):
        """Extracts 2D boundary condition lines from HDF file."""
        try:
            bc_lines_path = "Geometry/Boundary Condition Lines"
            if bc_lines_path not in hdf_file:
                return [], []
            
            # Get boundary condition line data
            bc_attrs = hdf_file[f"{bc_lines_path}/Attributes"][()]
            polyline_info = hdf_file[f"{bc_lines_path}/Polyline Info"][()]
            polyline_points = hdf_file[f"{bc_lines_path}/Polyline Points"][()]
            
            # Check if multi-part data exists
            has_parts = f"{bc_lines_path}/Polyline Parts" in hdf_file
            if has_parts:
                polyline_parts = hdf_file[f"{bc_lines_path}/Polyline Parts"][()]
            
            # Vectorized filtering of valid boundary condition lines
            valid_mask = polyline_info[:, 1] >= 2  # pnt_cnt >= 2
            valid_indices = np.where(valid_mask)[0]
            
            valid_data, geometries = [], []
            
            for idx in valid_indices:
                pnt_start, pnt_cnt, part_start, part_cnt = polyline_info[idx]
                attr_row = bc_attrs[idx]
                
                # Extract attributes
                name = attr_row["Name"]
                name = name.decode('utf-8', 'ignore').strip() if isinstance(name, bytes) else str(name)
                
                bc_type = attr_row["Type"]
                bc_type = bc_type.decode('utf-8', 'ignore').strip() if isinstance(bc_type, bytes) else str(bc_type)
                
                try:
                    # Extract points efficiently
                    points = polyline_points[pnt_start:pnt_start + pnt_cnt]
                    
                    if part_cnt == 1 or not has_parts:
                        # Single part - direct creation
                        arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in points])
                        geom = arcpy.Polyline(arcpy_array, sr)
                    else:
                        # Multi-part polyline
                        parts = polyline_parts[part_start:part_start + part_cnt]
                        all_parts_array = arcpy.Array()
                        
                        for part_pnt_start, part_pnt_cnt in parts:
                            if part_pnt_cnt > 1:
                                part_points = points[part_pnt_start:part_pnt_start + part_pnt_cnt]
                                part_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in part_points])
                                all_parts_array.add(part_array)
                        
                        if all_parts_array.count == 0:
                            continue
                        geom = arcpy.Polyline(all_parts_array, sr)
                    
                    valid_data.append({
                        'bc_id': int(idx),
                        'Name': name,
                        'Type': bc_type
                    })
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing boundary condition line {idx}: {str(e)}")
                    continue
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Boundary Condition Lines): {e}")
            raise arcpy.ExecuteError("Failed to read boundary condition lines from HDF file")

    def _get_pipe_conduits_direct(self, hdf_file, sr):
        """Extracts pipe conduits from HDF file with dynamic attributes."""
        try:
            conduits_path = "Geometry/Pipe Conduits"
            if conduits_path not in hdf_file:
                return [], []
            
            conduits_group = hdf_file[conduits_path]
            
            # Get attributes
            if 'Attributes' not in conduits_group:
                return [], []
            
            attributes = conduits_group['Attributes'][()]
            
            # Get polyline geometry data
            if 'Polyline Info' not in conduits_group or 'Polyline Points' not in conduits_group:
                return [], []
            
            polyline_info = conduits_group['Polyline Info'][()]
            polyline_points = conduits_group['Polyline Points'][()]
            
            valid_data, geometries = [], []
            
            # Debug: Show original field names
            if len(attributes) > 0:
                arcpy.AddMessage(f"DEBUG: Original HDF field names: {list(attributes.dtype.names)}")
            
            # Process each conduit
            for idx, (info, attr_row) in enumerate(zip(polyline_info, attributes)):
                point_start_idx, point_count = info[0], info[1]
                
                if point_count < 2:
                    continue
                
                try:
                    # Extract points for this conduit
                    coords = polyline_points[point_start_idx:point_start_idx + point_count]
                    
                    # Create polyline geometry
                    arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in coords])
                    geom = arcpy.Polyline(arcpy_array, sr)
                    
                    # Build attribute dictionary dynamically
                    attr_dict = {'conduit_id': int(idx)}
                    
                    # Process all attribute fields
                    attr_names = attributes.dtype.names
                    for field_name in attr_names:
                        value = attr_row[field_name]
                        
                        # Decode bytes to string if necessary
                        if isinstance(value, (bytes, np.bytes_)):
                            value = value.decode('utf-8', 'ignore').strip()
                        elif isinstance(value, np.ndarray) and value.dtype.kind == 'S':
                            value = value.tobytes().decode('utf-8', 'ignore').strip()
                        
                        # Clean field name for ArcGIS compatibility
                        clean_name = field_name.replace(' ', '_').replace(':', '_').replace(';', '_').replace(',', '_').replace('(', '_').replace(')', '_').replace("'", '')
                        
                        # Fix known typos in HDF field names
                        if 'Condtui_Connections' in clean_name:
                            clean_name = clean_name.replace('Condtui_Connections', 'Conduit_Connections')
                            if idx == 0:  # Only log for first record
                                arcpy.AddMessage(f"DEBUG: Fixed typo in field name 'Condtui_Connections' to 'Conduit_Connections'")
                        
                        # Special handling for exact "Shape" field (case insensitive)
                        if field_name.upper() == 'SHAPE':
                            clean_name = 'Shape_Type'
                            if idx == 0:
                                arcpy.AddMessage(f"DEBUG: Renamed 'Shape' field to 'Shape_Type' to avoid system field conflict")
                        # Rename other fields that conflict with system fields
                        elif clean_name.upper() in ['OBJECTID', 'SHAPE', 'SHAPE_LENGTH', 'SHAPE_AREA', 'SHAPE_LENG']:
                            original_clean = clean_name
                            clean_name = f"{clean_name}_USER"
                            if idx == 0:  # Only log for first record to avoid spam
                                arcpy.AddMessage(f"DEBUG: Renamed system field '{original_clean}' to '{clean_name}'")
                            
                        attr_dict[clean_name] = value
                    
                    valid_data.append(attr_dict)
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing pipe conduit {idx}: {str(e)}")
                    continue
            
            # Debug: Show cleaned field names from first record
            if valid_data:
                arcpy.AddMessage(f"DEBUG: Cleaned field names: {list(valid_data[0].keys())}")
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Could not read pipe conduits: {e}")
            return [], []

    def _get_pipe_nodes_direct(self, hdf_file, sr):
        """Extracts pipe nodes from HDF file with dynamic attributes."""
        try:
            nodes_path = "Geometry/Pipe Nodes"
            if nodes_path not in hdf_file:
                return [], []
            
            nodes_group = hdf_file[nodes_path]
            
            # Get attributes
            if 'Attributes' not in nodes_group:
                return [], []
            
            attributes = nodes_group['Attributes'][()]
            
            # Get points data
            if 'Points' not in nodes_group:
                return [], []
            
            points = nodes_group['Points'][()]
            
            valid_data, geometries = [], []
            
            # Debug: Show original field names
            if len(attributes) > 0:
                arcpy.AddMessage(f"DEBUG: Original HDF field names: {list(attributes.dtype.names)}")
            
            # Process each node
            for idx, (xy, attr_row) in enumerate(zip(points, attributes)):
                if len(xy) < 2:
                    continue
                
                try:
                    # Create point geometry
                    geom = arcpy.PointGeometry(arcpy.Point(xy[0], xy[1]), sr)
                    
                    # Build attribute dictionary dynamically
                    attr_dict = {'node_id': int(idx)}
                    
                    # Process all attribute fields
                    attr_names = attributes.dtype.names
                    for field_name in attr_names:
                        value = attr_row[field_name]
                        
                        # Decode bytes to string if necessary
                        if isinstance(value, (bytes, np.bytes_)):
                            value = value.decode('utf-8', 'ignore').strip()
                        elif isinstance(value, np.ndarray) and value.dtype.kind == 'S':
                            value = value.tobytes().decode('utf-8', 'ignore').strip()
                        
                        # Clean field name for ArcGIS compatibility
                        clean_name = field_name.replace(' ', '_').replace(':', '_').replace(';', '_').replace(',', '_').replace('(', '_').replace(')', '_').replace("'", '')
                        
                        # Fix known typos in HDF field names
                        if 'Condtui_Connections' in clean_name:
                            clean_name = clean_name.replace('Condtui_Connections', 'Conduit_Connections')
                            if idx == 0:  # Only log for first record
                                arcpy.AddMessage(f"DEBUG: Fixed typo in field name 'Condtui_Connections' to 'Conduit_Connections'")
                        
                        # Special handling for exact "Shape" field (case insensitive)
                        if field_name.upper() == 'SHAPE':
                            clean_name = 'Shape_Type'
                            if idx == 0:
                                arcpy.AddMessage(f"DEBUG: Renamed 'Shape' field to 'Shape_Type' to avoid system field conflict")
                        # Rename other fields that conflict with system fields
                        elif clean_name.upper() in ['OBJECTID', 'SHAPE', 'SHAPE_LENGTH', 'SHAPE_AREA', 'SHAPE_LENG']:
                            original_clean = clean_name
                            clean_name = f"{clean_name}_USER"
                            if idx == 0:  # Only log for first record to avoid spam
                                arcpy.AddMessage(f"DEBUG: Renamed system field '{original_clean}' to '{clean_name}'")
                            
                        attr_dict[clean_name] = value
                    
                    valid_data.append(attr_dict)
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing pipe node {idx}: {str(e)}")
                    continue
            
            # Debug: Show cleaned field names from first record
            if valid_data:
                arcpy.AddMessage(f"DEBUG: Cleaned field names: {list(valid_data[0].keys())}")
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Could not read pipe nodes: {e}")
            return [], []

    def _get_pipe_networks_direct(self, hdf_file, sr):
        """Extracts pipe network cell polygons from HDF file."""
        try:
            networks_path = "Geometry/Pipe Networks"
            if networks_path not in hdf_file:
                return [], []
            
            networks_group = hdf_file[networks_path]
            
            # Get network attributes
            if 'Attributes' not in networks_group:
                return [], []
            
            attributes = networks_group['Attributes'][()]
            if len(attributes) == 0:
                return [], []
            
            # Get the first network name (or could iterate through all)
            network_name = attributes[0]['Name']
            if isinstance(network_name, bytes):
                network_name = network_name.decode('utf-8', 'ignore').strip()
            
            arcpy.AddMessage(f"Processing pipe network: {network_name}")
            
            # Access the specific network group
            network_path = f"{networks_path}/{network_name}"
            if network_path not in hdf_file:
                arcpy.AddWarning(f"Network path '{network_path}' not found in HDF file")
                return [], []
            
            network_group = hdf_file[network_path]
            
            # Check for required datasets
            required_datasets = ['Cell Polygons Info', 'Cell Polygons Parts', 'Cell Polygons Points']
            for ds in required_datasets:
                if ds not in network_group:
                    arcpy.AddWarning(f"Required dataset '{ds}' not found in pipe network")
                    return [], []
            
            # Read cell polygon data
            cell_info = network_group['Cell Polygons Info'][()]
            cell_parts = network_group['Cell Polygons Parts'][()]
            cell_points = network_group['Cell Polygons Points'][()]
            
            # Read additional cell attributes if available
            cell_attributes = {}
            if 'Cell Property Table' in network_group:
                cell_property_table = network_group['Cell Property Table'][()]
                # Convert to dictionary for easier access
                for i, row in enumerate(cell_property_table):
                    cell_attributes[i] = {}
                    for field_name in cell_property_table.dtype.names:
                        value = row[field_name]
                        if isinstance(value, (bytes, np.bytes_)):
                            value = value.decode('utf-8', 'ignore').strip()
                        cell_attributes[i][field_name] = value
            
            # Read minimum elevations if available
            min_elevations = None
            if 'Cells Minimum Elevations' in network_group:
                min_elevations = network_group['Cells Minimum Elevations'][()]
            
            # Read node and conduit IDs if available
            node_conduit_ids = None
            if 'Cells Node and Conduit IDs' in network_group:
                node_conduit_ids = network_group['Cells Node and Conduit IDs'][()]
            
            valid_data, geometries = [], []
            
            # Process each cell
            for cell_idx, info in enumerate(cell_info):
                point_start_idx, point_count, part_start_idx, part_count = info
                
                try:
                    # Build polygon from parts
                    if part_count == 0:
                        continue
                    
                    parts_list = []
                    for p in range(part_start_idx, part_start_idx + part_count):
                        if p >= len(cell_parts):
                            continue
                        
                        part_info = cell_parts[p]
                        part_point_start = part_info[0]
                        part_point_count = part_info[1]
                        
                        # Extract coordinates for this part
                        coords = cell_points[part_point_start:part_point_start + part_point_count]
                        if len(coords) < 3:  # Need at least 3 points for a polygon
                            continue
                        
                        # Create arcpy array for this part
                        part_array = arcpy.Array([arcpy.Point(c[0], c[1]) for c in coords])
                        parts_list.append(part_array)
                    
                    if not parts_list:
                        continue
                    
                    # Create polygon geometry
                    if len(parts_list) == 1:
                        geom = arcpy.Polygon(parts_list[0], sr)
                    else:
                        # Multi-part polygon
                        all_parts = arcpy.Array()
                        for part in parts_list:
                            all_parts.add(part)
                        geom = arcpy.Polygon(all_parts, sr)
                    
                    # Build attribute dictionary
                    attr_dict = {
                        'cell_id': int(cell_idx),
                        'network_name': network_name
                    }
                    
                    # Add cell properties if available
                    if cell_idx in cell_attributes:
                        for key, value in cell_attributes[cell_idx].items():
                            # Clean field name
                            clean_key = key.replace(' ', '_').replace(':', '_').replace(';', '_').replace(',', '_')
                            attr_dict[clean_key] = value
                    
                    # Add minimum elevation if available
                    if min_elevations is not None and cell_idx < len(min_elevations):
                        attr_dict['min_elevation'] = float(min_elevations[cell_idx])
                    
                    # Add node and conduit IDs if available
                    if node_conduit_ids is not None and cell_idx < len(node_conduit_ids):
                        attr_dict['node_id'] = int(node_conduit_ids[cell_idx][0])
                        attr_dict['conduit_id'] = int(node_conduit_ids[cell_idx][1])
                    
                    valid_data.append(attr_dict)
                    geometries.append(geom)
                    
                except Exception as e:
                    arcpy.AddWarning(f"Error processing pipe network cell {cell_idx}: {str(e)}")
                    continue
            
            return valid_data, geometries
            
        except Exception as e:
            arcpy.AddWarning(f"Could not read pipe networks: {e}")
            return [], []

    def _get_mesh_areas_direct(self, hdf_file, sr):
        """Extracts mesh area perimeters from HDF file."""
        try:
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            raw_data = [{'mesh_name': name} for name in self._hdf_cache['mesh_names']]
            geometries = []
            
            flow_areas_path = "Geometry/2D Flow Areas"
            for mesh_name in self._hdf_cache['mesh_names']:
                perimeter_path = f"{flow_areas_path}/{mesh_name}/Perimeter"
                if perimeter_path in hdf_file:
                    coords = hdf_file[perimeter_path][()]
                    # Create polygon directly from numpy array
                    arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in coords])
                    geometries.append(arcpy.Polygon(arcpy_array, sr))
                else:
                    geometries.append(None)
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Perimeters): {e}")
            raise arcpy.ExecuteError()

    def _get_mesh_cell_points_direct(self, hdf_file, sr):
        """Extracts mesh cell centers using vectorized operations."""
        try:
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            raw_data, geometries = [], []
            
            for mesh_name in self._hdf_cache['mesh_names']:
                cell_centers_path = f"Geometry/2D Flow Areas/{mesh_name}/Cells Center Coordinate"
                if cell_centers_path not in hdf_file:
                    arcpy.AddWarning(f"No cell center data found for mesh '{mesh_name}'")
                    continue
                
                # Read all cell centers at once
                cell_centers = hdf_file[cell_centers_path][()]
                num_cells = len(cell_centers)
                
                # Vectorized data creation
                mesh_data = [{'mesh_name': mesh_name, 'cell_id': i} for i in range(num_cells)]
                raw_data.extend(mesh_data)
                
                # Batch create point geometries
                mesh_geometries = [arcpy.PointGeometry(arcpy.Point(coords[0], coords[1]), sr) 
                                 for coords in cell_centers]
                geometries.extend(mesh_geometries)
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Cell Points): {e}")
            raise arcpy.ExecuteError()

    def _get_mesh_cell_faces_direct(self, hdf_file, sr):
        """Extracts mesh cell faces with optimized coordinate handling."""
        try:
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            raw_data, geometries = [], []
            
            for mesh_name in self._hdf_cache['mesh_names']:
                try:
                    base = f"Geometry/2D Flow Areas/{mesh_name}"
                    
                    # Load all data at once
                    facepoints_index = hdf_file[f"{base}/Faces FacePoint Indexes"][()]
                    facepoints_coords = hdf_file[f"{base}/FacePoints Coordinate"][()]
                    faces_perim_info = hdf_file[f"{base}/Faces Perimeter Info"][()]
                    faces_perim_values = hdf_file[f"{base}/Faces Perimeter Values"][()]
                    
                    # Process faces in batches
                    for face_id, ((p_a, p_b), (s_row, count)) in enumerate(
                        zip(facepoints_index, faces_perim_info)):
                        
                        # Build coordinate array efficiently
                        if count > 0:
                            coords = np.vstack([
                                facepoints_coords[p_a:p_a+1],
                                faces_perim_values[s_row:s_row + count],
                                facepoints_coords[p_b:p_b+1]
                            ])
                        else:
                            coords = np.vstack([
                                facepoints_coords[p_a:p_a+1],
                                facepoints_coords[p_b:p_b+1]
                            ])
                        
                        # Create polyline
                        arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in coords])
                        geometries.append(arcpy.Polyline(arcpy_array, sr))
                        raw_data.append({'mesh_name': mesh_name, 'face_id': face_id})
                    
                except KeyError:
                    arcpy.AddWarning(f"No face data for mesh '{mesh_name}'.")
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Cell Faces): {e}")
            raise arcpy.ExecuteError()

    def _get_mesh_cells_direct(self, hdf_file, sr, precomputed_faces, messages):
        """
        Optimized mesh cell extraction using numpy arrays and pre-computed lookups.
        """
        try:
            messages.addMessage("Starting optimized cell polygon creation...")
            
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            # Build optimized face lookup with numpy arrays
            face_lookup = {}
            face_arrays = {}  # Store face coordinates as numpy arrays
            
            for i, (face_attr, face_geom) in enumerate(zip(precomputed_faces[0], precomputed_faces[1])):
                mesh_name = face_attr['mesh_name']
                face_id = face_attr['face_id']
                
                if mesh_name not in face_lookup:
                    face_lookup[mesh_name] = {}
                    face_arrays[mesh_name] = {}
                
                face_lookup[mesh_name][face_id] = face_geom
                
                # Store coordinates as numpy array for faster access
                if face_geom and hasattr(face_geom, 'getPart'):
                    part = face_geom.getPart(0)
                    coords = np.array([[part.getObject(i).X, part.getObject(i).Y]
                                     for i in range(part.count) if part.getObject(i)])
                    if len(coords) > 0:
                        face_arrays[mesh_name][face_id] = coords
            
            raw_data, geometries = [], []
            total_cells_processed = 0
            
            for mesh_name in self._hdf_cache['mesh_names']:
                messages.addMessage(f"\nProcessing mesh '{mesh_name}'...")
                
                try:
                    base = f"Geometry/2D Flow Areas/{mesh_name}"
                    
                    # Load cell-face relationships
                    cell_face_info = hdf_file[f"{base}/Cells Face and Orientation Info"][()]
                    cell_face_values = hdf_file[f"{base}/Cells Face and Orientation Values"][()]
                    
                    # Extract as numpy arrays for efficiency
                    face_indices = cell_face_values[:, 0].astype(np.int32)
                    orientations = cell_face_values[:, 1].astype(np.int32)
                    
                    mesh_faces = face_lookup.get(mesh_name, {})
                    mesh_face_arrays = face_arrays.get(mesh_name, {})
                    
                    num_cells = len(cell_face_info)
                    cells_created = 0
                    
                    # Process in batches for progress reporting
                    batch_size = 5000
                    
                    for batch_start in range(0, num_cells, batch_size):
                        batch_end = min(batch_start + batch_size, num_cells)
                        batch_created = 0
                        
                        for cell_id in range(batch_start, batch_end):
                            start, length = cell_face_info[cell_id]
                            
                            if length < 3:  # Need at least 3 faces
                                continue
                            
                            # Get face indices for this cell
                            cell_face_ids = face_indices[start:start + length]
                            cell_orientations = orientations[start:start + length]
                            
                            # Collect face geometries
                            face_geoms = []
                            
                            for j, (face_id, orientation) in enumerate(zip(cell_face_ids, cell_orientations)):
                                if face_id in mesh_faces:
                                    face_geom = mesh_faces[face_id]
                                    
                                    # Handle orientation
                                    if orientation < 0 and face_id in mesh_face_arrays:
                                        # Create reversed geometry using numpy array
                                        coords = mesh_face_arrays[face_id][::-1]
                                        arcpy_array = arcpy.Array([arcpy.Point(x, y) for x, y in coords])
                                        face_geom = arcpy.Polyline(arcpy_array, sr)
                                    
                                    if face_geom:
                                        face_geoms.append(face_geom)
                            
                            if len(face_geoms) >= 3:
                                # Use optimized polygon construction
                                polygon = polygonize_arcpy_optimized(face_geoms, sr)
                                
                                if polygon and polygon.area > 0:
                                    raw_data.append({'mesh_name': mesh_name, 'cell_id': cell_id})
                                    geometries.append(polygon)
                                    cells_created += 1
                                    batch_created += 1
                                    total_cells_processed += 1
                        
                        # Progress update
                        if batch_end % 10000 == 0 or batch_end == num_cells:
                            messages.addMessage(
                                f"  Processed {batch_end}/{num_cells} cells in mesh '{mesh_name}' "
                                f"({cells_created} valid polygons)"
                            )
                    
                    messages.addMessage(
                        f"  Completed mesh '{mesh_name}': {cells_created} cells created"
                    )
                    
                except Exception as e:
                    messages.addErrorMessage(f"Error processing mesh '{mesh_name}': {str(e)}")
                    continue
            
            messages.addMessage(f"\nTotal cells processed: {total_cells_processed}")
            return raw_data, geometries
            
        except Exception as e:
            messages.addErrorMessage(f"Fatal error in cell creation: {str(e)}")
            raise arcpy.ExecuteError()

    # --- Main Execution Logic ---
    def execute(self, parameters, messages):
        hdf_path = parameters[0].valueAsText
        
        # Get selected elements
        geometry_elements = parameters[2].values if parameters[2].values else []
        
        if not geometry_elements:
            messages.addErrorMessage("No geometry elements selected for loading. Please select at least one element.")
            raise arcpy.ExecuteError
        
        # Get geodatabase parameters
        output_gdb = parameters[12].valueAsText
        create_gdb = parameters[13].value
        output_workspace = None
        
        # Extract project and plan info
        project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
        
        # Get projection
        proj_wkt = get_ras_projection_wkt(hdf_path)
        sr = None
        if proj_wkt:
            sr = arcpy.SpatialReference()
            sr.loadFromString(proj_wkt)
            messages.addMessage(f"CRS '{sr.name}' found in HEC-RAS project files.")
        elif parameters[1].value:
            sr = parameters[1].value
            messages.addMessage(f"Using user-defined override CRS: {sr.name}")
        else:
            messages.addErrorMessage("CRS could not be determined. Please use the Override CRS parameter.")
            raise arcpy.ExecuteError
        
        # Setup geodatabase
        if create_gdb or output_gdb:
            if create_gdb and not output_gdb:
                # Auto-create geodatabase based on HDF name
                output_gdb = create_geodatabase_from_hdf(hdf_path, messages)
            
            # Create feature dataset with project/plan naming
            feature_dataset_name = get_feature_dataset_name(hdf_path)
            output_workspace = setup_geodatabase_output(output_gdb, feature_dataset_name, sr, messages)
            messages.addMessage(f"Output workspace set to: {output_workspace}")
        
        # Open HDF file once and cache metadata
        with h5py.File(hdf_path, 'r') as hdf_file:
            messages.addMessage("Caching HDF metadata...")
            self._hdf_cache = cache_hdf_metadata(hdf_file)
            
            # Pre-compute faces if needed
            precomputed_faces = None
            if (self.CELL_FACES in geometry_elements or 
                self.CELL_POLYS in geometry_elements):
                messages.addMessage("Pre-loading Mesh Cell Faces...")
                precomputed_faces = self._get_mesh_cell_faces_direct(hdf_file, sr)
            
            # Process geometry elements
            if self.BREAKLINES in geometry_elements and parameters[3].valueAsText:
                output_fc = parameters[3].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "Breaklines2D"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[3].value = output_fc
                
                data, geoms = self._get_breaklines_direct(hdf_file, sr)
                fields = [("bl_id", "LONG"), ("Name", "TEXT"), ("CellSpaceNear", "FLOAT"), 
                         ("CellSpaceFar", "FLOAT"), ("NearRepeats", "LONG"), ("ProtectRadius", "LONG")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "2D breaklines with cell spacing attributes", hdf_path)
            
            if self.BC_LINES in geometry_elements and parameters[4].valueAsText:
                output_fc = parameters[4].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "BCLines2D"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[4].value = output_fc
                
                data, geoms = self._get_bc_lines_direct(hdf_file, sr)
                fields = [("bc_id", "LONG"), ("Name", "TEXT"), ("Type", "TEXT")]
                write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "2D boundary condition lines", hdf_path)
            
            if self.PERIMETERS in geometry_elements and parameters[5].valueAsText:
                output_fc = parameters[5].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MeshPerimeters"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[5].value = output_fc
                
                data, geoms = self._get_mesh_areas_direct(hdf_file, sr)
                fields = [("mesh_name", "TEXT")]
                write_features_to_fc(output_fc, sr, "POLYGON", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "2D flow area perimeter polygons", hdf_path)
            
            if self.CELL_POINTS in geometry_elements and parameters[6].valueAsText:
                output_fc = parameters[6].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MeshCellCenters"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[6].value = output_fc
                
                data, geoms = self._get_mesh_cell_points_direct(hdf_file, sr)
                fields = [("mesh_name", "TEXT"), ("cell_id", "LONG")]
                write_features_to_fc(output_fc, sr, "POINT", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Mesh cell center points", hdf_path)
            
            if self.CELL_FACES in geometry_elements and parameters[7].valueAsText:
                output_fc = parameters[7].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MeshCellFaces"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[7].value = output_fc
                
                if precomputed_faces:
                    data, geoms = precomputed_faces
                    fields = [("mesh_name", "TEXT"), ("face_id", "LONG")]
                    write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                    if output_workspace and data:
                        add_feature_class_metadata(output_fc, "Mesh cell face polylines", hdf_path)
            
            if self.CELL_POLYS in geometry_elements and parameters[8].valueAsText:
                output_fc = parameters[8].valueAsText
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MeshCellPolygons"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[8].value = output_fc
                
                if precomputed_faces:
                    messages.addMessage("Constructing cell polygons from faces...")
                    data, geoms = self._get_mesh_cells_direct(hdf_file, sr, precomputed_faces, messages)
                    fields = [("mesh_name", "TEXT"), ("cell_id", "LONG")]
                    write_features_to_fc(output_fc, sr, "POLYGON", fields, data, geoms, messages)
                    if output_workspace and data:
                        add_feature_class_metadata(output_fc, "Mesh cell polygons", hdf_path)
            
            # Process pipe network elements
            # Pipe networks use the same feature dataset as other geometry
            pipe_workspace = output_workspace
            
            if self.PIPE_CONDUITS in geometry_elements and parameters[9].valueAsText:
                # Check cache first
                if not self._hdf_cache.get('has_pipe_conduits', False):
                    messages.addMessage("No pipe conduits found in the HDF file.")
                else:
                    output_fc = parameters[9].valueAsText
                    # Update output path if using geodatabase
                    if pipe_workspace:
                        fc_name = "PipeConduits"
                        output_fc = os.path.join(pipe_workspace, fc_name)
                        parameters[9].value = output_fc
                    
                    messages.addMessage("Extracting Pipe Conduits...")
                    data, geoms = self._get_pipe_conduits_direct(hdf_file, sr)
                    if data:
                        # Get dynamic fields from the data
                        fields = get_dynamic_fields_from_data(data)
                        write_features_to_fc(output_fc, sr, "POLYLINE", fields, data, geoms, messages)
                        if pipe_workspace:
                            add_feature_class_metadata(output_fc, "Pipe conduits (storm/sewer networks)", hdf_path)
                    else:
                        messages.addMessage("No pipe conduits data extracted.")
            
            if self.PIPE_NODES in geometry_elements and parameters[10].valueAsText:
                # Check cache first
                if not self._hdf_cache.get('has_pipe_nodes', False):
                    messages.addMessage("No pipe nodes found in the HDF file.")
                else:
                    output_fc = parameters[10].valueAsText
                    # Update output path if using geodatabase
                    if pipe_workspace:
                        fc_name = "PipeNodes"
                        output_fc = os.path.join(pipe_workspace, fc_name)
                        parameters[10].value = output_fc
                    
                    messages.addMessage("Extracting Pipe Nodes...")
                    data, geoms = self._get_pipe_nodes_direct(hdf_file, sr)
                    if data:
                        # Get dynamic fields from the data
                        fields = get_dynamic_fields_from_data(data)
                        write_features_to_fc(output_fc, sr, "POINT", fields, data, geoms, messages)
                        if pipe_workspace:
                            add_feature_class_metadata(output_fc, "Pipe junction nodes", hdf_path)
                    else:
                        messages.addMessage("No pipe nodes data extracted.")
            
            if self.PIPE_NETWORKS in geometry_elements and parameters[11].valueAsText:
                # Check cache first - pipe networks might not have a specific cache flag
                output_fc = parameters[11].valueAsText
                # Update output path if using geodatabase
                if pipe_workspace:
                    fc_name = "PipeNetworks"
                    output_fc = os.path.join(pipe_workspace, fc_name)
                    parameters[11].value = output_fc
                
                messages.addMessage("Extracting Pipe Networks...")
                data, geoms = self._get_pipe_networks_direct(hdf_file, sr)
                if data:
                    # Get dynamic fields from the data
                    fields = get_dynamic_fields_from_data(data)
                    write_features_to_fc(output_fc, sr, "POLYGON", fields, data, geoms, messages)
                    if pipe_workspace:
                        add_feature_class_metadata(output_fc, "Pipe network elements", hdf_path)
                else:
                    messages.addMessage("No pipe networks data extracted.")
        
        messages.addMessage("\nProcessing complete.")
        return
    
    def getHelp(self, tool_name):
        """Return help documentation URL for the tool."""
        help_file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
                                "Doc", "RASCommander_Help.html")
        if os.path.exists(help_file):
            return f"file:///{help_file.replace(os.sep, '/')}#load-hec-ras-2d-geometry-layers"
        return None
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_load_hecras_2d_results.py
==================================================
# -*- coding: utf-8 -*-
"""
LoadHECRAS2DResults.py

Tool for loading HEC-RAS 2D results summary layers from HDF files including
maximum water surface elevation and face velocities.
"""

import arcpy
import os
import h5py
import numpy as np
from datetime import datetime, timedelta

# Import helper functions from utils
from rc_utils import (
    get_ras_projection_wkt,
    get_polyline_centroid_vectorized,
    cache_hdf_metadata,
    write_features_to_fc,
    setup_geodatabase_output,
    get_unique_fc_name,
    add_feature_class_metadata,
    extract_project_and_plan_info,
    create_geodatabase_from_hdf,
    get_feature_dataset_name,
    get_feature_class_name
)


class LoadHECRAS2DResults(object):
    """
    Loads 2D results summary data from a HEC-RAS HDF file.
    """
    def __init__(self):
        self.label = "Load HEC-RAS 2D Results Summary Layers"
        self.description = """Extracts 2D results summary data from a HEC-RAS HDF file including maximum water surface elevation and face velocities.
        
        This tool extracts summary results data from HEC-RAS plan files (p*.hdf) that contain simulation results.
        
        Available results include:
        • Max WSE at Cell Centers - Maximum water surface elevation achieved at each cell center during the simulation, with the time of occurrence
        • Max Vel at Cell Faces - Maximum velocity achieved at each cell face during the simulation, with the time of occurrence
        
        Both results types create point feature classes with attributes for the maximum value and the time when it occurred.
        
        Note: This tool requires a plan HDF file that contains results data. Geometry-only files will not work."""
        self.canRunInBackground = False
        
        # Results elements
        self.MAX_WSE_POINTS = "Max WSE at Cell Centers"
        self.MAX_FACE_VEL_POINTS = "Max Vel at Cell Faces"
        
        # Cache for HDF metadata
        self._hdf_cache = {}

    def getParameterInfo(self):
        results_elements = [self.MAX_WSE_POINTS, self.MAX_FACE_VEL_POINTS]

        params = [
            arcpy.Parameter(displayName="Plan HDF File with Results", name="input_hdf", datatype="DEFile", 
                          parameterType="Required", direction="Input"),
            arcpy.Parameter(displayName="Override CRS (Optional)", name="override_crs", datatype="GPSpatialReference", 
                          parameterType="Optional", direction="Input"),
            
            # Results elements to load
            arcpy.Parameter(displayName="Results to Load", name="results_elements", datatype="GPString", 
                          parameterType="Required", direction="Input", multiValue=True),
            
            # Output parameters
            arcpy.Parameter(displayName="Output Max WSE at Cell Centers", name="output_max_wse", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            arcpy.Parameter(displayName="Output Max Vel at Cell Faces", name="output_max_face_vel", datatype="DEFeatureClass", 
                          parameterType="Optional", direction="Output", category="Outputs"),
            
            # Geodatabase organization parameters
            arcpy.Parameter(displayName="Output Geodatabase (Optional)", name="output_gdb", datatype="DEWorkspace", 
                          parameterType="Optional", direction="Output", category="Output"),
            arcpy.Parameter(displayName="Create New Geodatabase", name="create_gdb", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input", category="Output")
        ]
        
        # Configure HDF file filter
        params[0].filter.list = ["hdf", "p*.hdf"]
        params[0].description = """Select a HEC-RAS plan file (p*.hdf) that contains simulation results. 
        Geometry-only files will not contain the required results data."""
        
        params[1].description = """Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files. 
        The tool will first attempt to read the CRS from the HDF file or associated .prj files."""
        
        # Set filters for multi-value parameters
        params[2].filter.type = "ValueList"
        params[2].filter.list = results_elements
        params[2].value = [self.MAX_WSE_POINTS]  # Default selection
        params[2].description = """Select one or more results types to extract from the HDF file. 
        Each selected result will create a separate output feature class."""
        
        # Set default output paths and descriptions
        params[3].value = r"memory\MaximumWSE"
        params[3].description = """Output feature class for maximum water surface elevation points. 
        Includes attributes for cell ID, mesh name, maximum WSE value, and time of occurrence."""
        
        params[4].value = r"memory\MaximumFaceVelocity"
        params[4].description = """Output feature class for maximum face velocity points. 
        Includes attributes for face ID, mesh name, maximum velocity value, and time of occurrence."""
        
        # Geodatabase parameters
        params[5].description = """Specify a geodatabase to organize all output feature classes. 
        If provided, outputs will be created in this geodatabase instead of the default locations."""
        
        params[6].value = True  # Default to creating new geodatabase
        params[6].description = """Create a new geodatabase based on the HDF file name. 
        The geodatabase will be named using the pattern: ProjectName.pXX.gdb"""
        
        return params

    def isLicensed(self):
        """Set whether tool is licensed to execute."""
        return True

    def updateParameters(self, parameters):
        """Modify the values and properties of parameters before internal validation."""
        # Enable/disable output parameters based on selected elements
        if parameters[2].value:
            selected = parameters[2].valueAsText.split(';') if parameters[2].valueAsText else []
            
            # Enable/disable outputs based on selection
            parameters[3].enabled = self.MAX_WSE_POINTS in selected
            parameters[4].enabled = self.MAX_FACE_VEL_POINTS in selected
        
        # Auto-populate geodatabase path when HDF file is selected
        if parameters[0].value and parameters[0].altered:  # input_hdf
            hdf_path = parameters[0].valueAsText
            
            # If create_gdb is True, auto-populate geodatabase path
            if parameters[6].value:  # create_gdb
                project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
                gdb_name = f"{base_name}.gdb"
                gdb_path = os.path.join(os.path.dirname(hdf_path), gdb_name)
                parameters[5].value = gdb_path
        
        return
    
    def updateMessages(self, parameters):
        """Modify messages created by internal validation."""
        # Check if HDF file is plan file with results
        if parameters[0].value:
            hdf_path = parameters[0].valueAsText
            if hdf_path and os.path.basename(hdf_path).lower().startswith('g'):
                parameters[0].setWarningMessage(
                    "This appears to be a geometry file (g*.hdf). Results data is typically in plan files (p*.hdf)."
                )
        
        # Clear geodatabase validation error if create_gdb is True
        if parameters[6].value and parameters[5].hasError():  # create_gdb and output_gdb has error
            parameters[5].clearMessage()
        
        return

    # --- HDF Data Extraction Methods ---

    def _get_max_wse_points_direct(self, hdf_file, sr):
        """Extracts maximum water surface elevation points with vectorized operations."""
        try:
            if not self._hdf_cache['has_results']:
                arcpy.AddError("No results data found in HDF file.")
                return [], []
            
            raw_data, geometries = [], []
            start_time = self._hdf_cache['simulation_start_time']
            
            arcpy.AddMessage(f'Simulation start time: {start_time}')
            
            for mesh_name in self._hdf_cache['mesh_names']:
                arcpy.AddMessage(f'Processing max WSE for mesh: {mesh_name}')
                
                # Get cell centers
                centers_path = f"Geometry/2D Flow Areas/{mesh_name}/Cells Center Coordinate"
                if centers_path not in hdf_file:
                    arcpy.AddWarning(f"No cell centers found for mesh '{mesh_name}'. Skipping.")
                    continue
                
                cell_centers = hdf_file[centers_path][()]
                
                # Get maximum water surface data
                summary_path = f"Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas/{mesh_name}/Maximum Water Surface"
                if summary_path not in hdf_file:
                    arcpy.AddWarning(f"No 'Maximum Water Surface' data for mesh '{mesh_name}'. Skipping.")
                    continue
                
                max_wse_data = hdf_file[summary_path][:]
                
                # Data is 2D array: row 0 = values, row 1 = times (in days)
                if max_wse_data.ndim == 2 and max_wse_data.shape[0] == 2:
                    wse_values = max_wse_data[0, :]
                    time_in_days = max_wse_data[1, :]
                else:
                    arcpy.AddWarning(f"Unexpected data format for mesh '{mesh_name}'. Skipping.")
                    continue
                
                num_cells = min(len(cell_centers), len(wse_values))
                
                # Vectorized time conversion
                time_deltas = np.array([timedelta(days=float(t)) for t in time_in_days[:num_cells]])
                times_of_max = [start_time + td for td in time_deltas]
                
                # Create data and geometries in batches
                mesh_data = [{
                    'mesh_name': mesh_name,
                    'cell_id': i,
                    'max_wse': float(wse_values[i]),
                    'max_wse_time': times_of_max[i]
                } for i in range(num_cells)]
                
                mesh_geometries = [
                    arcpy.PointGeometry(arcpy.Point(cell_centers[i][0], cell_centers[i][1]), sr)
                    for i in range(num_cells)
                ]
                
                raw_data.extend(mesh_data)
                geometries.extend(mesh_geometries)
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Max WSE Points): {e}")
            raise arcpy.ExecuteError()

    def _get_mesh_cell_faces_direct(self, hdf_file, sr):
        """Extracts mesh cell faces needed for face velocity calculation."""
        try:
            if not self._hdf_cache['mesh_names']:
                return [], []
            
            raw_data, geometries = [], []
            
            for mesh_name in self._hdf_cache['mesh_names']:
                try:
                    base = f"Geometry/2D Flow Areas/{mesh_name}"
                    
                    # Load all data at once
                    facepoints_index = hdf_file[f"{base}/Faces FacePoint Indexes"][()]
                    facepoints_coords = hdf_file[f"{base}/FacePoints Coordinate"][()]
                    faces_perim_info = hdf_file[f"{base}/Faces Perimeter Info"][()]
                    faces_perim_values = hdf_file[f"{base}/Faces Perimeter Values"][()]
                    
                    # Process faces in batches
                    for face_id, ((p_a, p_b), (s_row, count)) in enumerate(
                        zip(facepoints_index, faces_perim_info)):
                        
                        # Build coordinate array efficiently
                        if count > 0:
                            coords = np.vstack([
                                facepoints_coords[p_a:p_a+1],
                                faces_perim_values[s_row:s_row + count],
                                facepoints_coords[p_b:p_b+1]
                            ])
                        else:
                            coords = np.vstack([
                                facepoints_coords[p_a:p_a+1],
                                facepoints_coords[p_b:p_b+1]
                            ])
                        
                        # Create polyline
                        arcpy_array = arcpy.Array([arcpy.Point(p[0], p[1]) for p in coords])
                        geometries.append(arcpy.Polyline(arcpy_array, sr))
                        raw_data.append({'mesh_name': mesh_name, 'face_id': face_id})
                    
                except KeyError:
                    arcpy.AddWarning(f"No face data for mesh '{mesh_name}'.")
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Cell Faces): {e}")
            raise arcpy.ExecuteError()

    def _get_max_face_velocity_points_direct(self, hdf_file, sr):
        """Extracts maximum face velocity points with optimized centroid calculation."""
        try:
            if not self._hdf_cache['has_results']:
                arcpy.AddError("No results data found in HDF file.")
                return [], []
            
            raw_data, geometries = [], []
            start_time = self._hdf_cache['simulation_start_time']
            
            arcpy.AddMessage(f'Simulation start time: {start_time}')
            
            # Get face geometries
            face_data, face_geoms = self._get_mesh_cell_faces_direct(hdf_file, sr)
            
            # Build lookup
            face_lookup = {}
            for i, (face_attr, face_geom) in enumerate(zip(face_data, face_geoms)):
                mesh_name = face_attr['mesh_name']
                face_id = face_attr['face_id']
                
                if mesh_name not in face_lookup:
                    face_lookup[mesh_name] = {}
                
                face_lookup[mesh_name][face_id] = face_geom
            
            for mesh_name in self._hdf_cache['mesh_names']:
                arcpy.AddMessage(f'Processing max face velocity for mesh: {mesh_name}')
                
                # Get maximum face velocity data
                summary_path = f"Results/Unsteady/Output/Output Blocks/Base Output/Summary Output/2D Flow Areas/{mesh_name}/Maximum Face Velocity"
                if summary_path not in hdf_file:
                    arcpy.AddWarning(f"No 'Maximum Face Velocity' data for mesh '{mesh_name}'. Skipping.")
                    continue
                
                max_vel_data = hdf_file[summary_path][:]
                
                # Data is 2D array: row 0 = values, row 1 = times (in days)
                if max_vel_data.ndim == 2 and max_vel_data.shape[0] == 2:
                    vel_values = max_vel_data[0, :]
                    time_in_days = max_vel_data[1, :]
                else:
                    arcpy.AddWarning(f"Unexpected data format for mesh '{mesh_name}'. Skipping.")
                    continue
                
                # Process faces
                mesh_faces = face_lookup.get(mesh_name, {})
                
                for face_id in range(len(vel_values)):
                    if face_id not in mesh_faces:
                        continue
                    
                    face_geom = mesh_faces[face_id]
                    if not face_geom:
                        continue
                    
                    # Calculate centroid using optimized function
                    centroid_pt = get_polyline_centroid_vectorized(face_geom)
                    if not centroid_pt:
                        continue
                    
                    # Convert time
                    time_of_max = start_time + timedelta(days=float(time_in_days[face_id]))
                    
                    raw_data.append({
                        'mesh_name': mesh_name,
                        'face_id': face_id,
                        'max_vel': float(vel_values[face_id]),
                        'time_of_max': time_of_max
                    })
                    
                    geometries.append(arcpy.PointGeometry(centroid_pt, sr))
            
            return raw_data, geometries
            
        except Exception as e:
            arcpy.AddError(f"HDF Read Error (Max Face Velocity Points): {e}")
            raise arcpy.ExecuteError()

    # --- Main Execution Logic ---
    def execute(self, parameters, messages):
        hdf_path = parameters[0].valueAsText
        
        # Get selected elements
        results_elements = parameters[2].values if parameters[2].values else []
        
        if not results_elements:
            messages.addErrorMessage("No results elements selected for loading. Please select at least one element.")
            raise arcpy.ExecuteError
        
        # Get geodatabase parameters
        output_gdb = parameters[5].valueAsText if len(parameters) > 5 else None
        create_gdb = parameters[6].value if len(parameters) > 6 else False
        output_workspace = None
        
        # Extract project and plan info
        project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
        
        # Get projection
        proj_wkt = get_ras_projection_wkt(hdf_path)
        sr = None
        if proj_wkt:
            sr = arcpy.SpatialReference()
            sr.loadFromString(proj_wkt)
            messages.addMessage(f"CRS '{sr.name}' found in HEC-RAS project files.")
        elif parameters[1].value:
            sr = parameters[1].value
            messages.addMessage(f"Using user-defined override CRS: {sr.name}")
        else:
            messages.addErrorMessage("CRS could not be determined. Please use the Override CRS parameter.")
            raise arcpy.ExecuteError
        
        # Setup geodatabase
        if create_gdb or output_gdb:
            if create_gdb and not output_gdb:
                # Auto-create geodatabase based on HDF name
                output_gdb = create_geodatabase_from_hdf(hdf_path, messages)
            
            # Create feature dataset with project/plan naming
            feature_dataset_name = get_feature_dataset_name(hdf_path)
            output_workspace = setup_geodatabase_output(output_gdb, feature_dataset_name, sr, messages)
            messages.addMessage(f"Output workspace set to: {output_workspace}")
        
        # Open HDF file once and cache metadata
        with h5py.File(hdf_path, 'r') as hdf_file:
            messages.addMessage("Caching HDF metadata...")
            self._hdf_cache = cache_hdf_metadata(hdf_file)
            
            # Check if results exist
            if not self._hdf_cache['has_results']:
                messages.addErrorMessage("No results data found in the HDF file. Please ensure this is a plan HDF file with results.")
                raise arcpy.ExecuteError
            
            # Process results elements
            if self.MAX_WSE_POINTS in results_elements and parameters[3].valueAsText:
                output_fc = parameters[3].valueAsText
                
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MaxWSE"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[3].value = output_fc
                
                messages.addMessage("Extracting Maximum Water Surface Elevation points...")
                data, geoms = self._get_max_wse_points_direct(hdf_file, sr)
                fields = [("mesh_name", "TEXT"), ("cell_id", "LONG"), ("max_wse", "DOUBLE"), 
                         ("max_wse_time", "DATE")]
                write_features_to_fc(output_fc, sr, "POINT", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Maximum water surface elevation at cell centers", hdf_path)
            
            if self.MAX_FACE_VEL_POINTS in results_elements and parameters[4].valueAsText:
                output_fc = parameters[4].valueAsText
                
                # Update output path if using geodatabase
                if output_workspace:
                    fc_name = "MaxVelocity"
                    output_fc = os.path.join(output_workspace, fc_name)
                    parameters[4].value = output_fc
                
                messages.addMessage("Extracting Maximum Face Velocity points...")
                data, geoms = self._get_max_face_velocity_points_direct(hdf_file, sr)
                fields = [("mesh_name", "TEXT"), ("face_id", "LONG"), ("max_vel", "DOUBLE"), 
                         ("time_of_max", "DATE")]
                write_features_to_fc(output_fc, sr, "POINT", fields, data, geoms, messages)
                if output_workspace and data:
                    add_feature_class_metadata(output_fc, "Maximum velocity at cell faces", hdf_path)
        
        messages.addMessage("\nProcessing complete.")
        return
    
    def getHelp(self, tool_name):
        """Return help documentation URL for the tool."""
        help_file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
                                "Doc", "RASCommander_Help.html")
        if os.path.exists(help_file):
            return f"file:///{help_file.replace(os.sep, '/')}#load-hec-ras-2d-results-summary-layers"
        return None
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_load_ras_terrain.py
==================================================
# -*- coding: utf-8 -*-
"""
LoadRASTerrain.py

Tool for loading HEC-RAS terrain layers from a project's rasmap file.
"""

import arcpy
import os
import xml.etree.ElementTree as ET
from pathlib import Path


class LoadRASTerrain(object):
    """
    Loads one or more HEC-RAS terrain layers from a project's rasmap file.
    
    IMPORTANT: This tool loads the underlying terrain TIFFs as a VRT (Virtual Raster) 
    with the priority from HEC-RAS in place. It does NOT include terrain modifications 
    done as vector terrain modifications in RAS Mapper. Only the base terrain raster 
    data will be loaded.
    """
    def __init__(self):
        self.label = "Load HEC-RAS Terrain"
        self.description = """Loads terrain layers defined in a HEC-RAS project's .rasmap file into the current map.
        
        ⚠️ IMPORTANT REMINDER: The loaded terrain layers are base VRT files only. Any vector terrain 
        modifications (breaklines, high ground, etc.) made in RAS Mapper are NOT included in these layers."""
        self.canRunInBackground = False
        self._terrain_cache = {}

    def getParameterInfo(self):
        params = [
            arcpy.Parameter(
                displayName="HEC-RAS Project File (*.prj)",
                name="in_ras_project",
                datatype="DEFile",
                parameterType="Required",
                direction="Input"
            ),
            arcpy.Parameter(
                displayName="Import All Terrains",
                name="import_all",
                datatype="GPBoolean",
                parameterType="Optional",
                direction="Input"
            ),
            arcpy.Parameter(
                displayName="Terrains to Load",
                name="terrains_to_load",
                datatype="GPString",
                parameterType="Optional",
                direction="Input",
                multiValue=True
            )
        ]
        
        params[0].filter.list = ["prj"]
        params[0].description = """Select the HEC-RAS project file (*.prj). 
        The tool will automatically find the associated .rasmap file in the same directory."""
        
        params[1].value = False
        params[1].description = """Check this box to import all terrain layers found in the project. 
        When checked, the terrain selection list will be disabled."""
        
        params[2].filter.type = "ValueList"
        params[2].description = """Select specific terrain layers to load. 
        This list is populated from the terrains found in the .rasmap file. 
        Disabled when 'Import All Terrains' is checked."""
        
        return params
    
    def updateMessages(self, parameters):
        """Modify the messages created by internal parameter validation."""
        return

    def isLicensed(self):
        return True

    def _get_rasmap_path_from_prj(self, prj_path_str):
        """Finds the .rasmap file associated with a .prj file."""
        if not prj_path_str or not os.path.exists(prj_path_str):
            return None
        
        p = Path(prj_path_str)
        rasmap_path = p.with_suffix('.rasmap')
        
        if rasmap_path.exists():
            return str(rasmap_path)
        
        arcpy.AddWarning(f"Could not find associated .rasmap file for {p.name}")
        return None

    def _get_terrain_info_from_rasmap(self, rasmap_path_str):
        """Parses a .rasmap file to get terrain names and their HDF file paths."""
        if not rasmap_path_str or not os.path.exists(rasmap_path_str):
            return {}
            
        terrains = {}
        try:
            tree = ET.parse(rasmap_path_str)
            root = tree.getroot()
            project_folder = os.path.dirname(rasmap_path_str)
            
            for layer in root.findall(".//Terrains/Layer"):
                name = layer.get('Name')
                filename_rel = layer.get('Filename')
                
                if name and filename_rel:
                    # HEC-RAS paths can start with '.\', remove it for robust joining
                    clean_rel_path = filename_rel.lstrip('.\\/')
                    filename_abs = os.path.join(project_folder, clean_rel_path)
                    terrains[name] = os.path.normpath(filename_abs)
                    
        except ET.ParseError as e:
            arcpy.AddWarning(f"Error parsing .rasmap file {os.path.basename(rasmap_path_str)}: {e}")
        except Exception as e:
            arcpy.AddWarning(f"An unexpected error occurred while reading the .rasmap file: {e}")
            
        return terrains

    def updateParameters(self, parameters):
        """Modify the parameters on the GUI according to user input."""
        if parameters[0].value and parameters[0].altered:
            prj_path = parameters[0].valueAsText
            rasmap_path = self._get_rasmap_path_from_prj(prj_path)
            
            if rasmap_path:
                self._terrain_cache = self._get_terrain_info_from_rasmap(rasmap_path)
                terrain_names = list(self._terrain_cache.keys())
                parameters[2].filter.list = sorted(terrain_names)
            else:
                parameters[2].filter.list = []
                self._terrain_cache = {}

        if parameters[1].value is True:
            parameters[2].enabled = False
            parameters[2].value = None # Clear selection if "Import All" is checked
        else:
            parameters[2].enabled = True
        return

    def execute(self, parameters, messages):
        """The source code of the tool."""
        prj_path = parameters[0].valueAsText
        import_all = parameters[1].value
        selected_terrains = parameters[2].values

        if not self._terrain_cache:
            rasmap_path = self._get_rasmap_path_from_prj(prj_path)
            if rasmap_path:
                self._terrain_cache = self._get_terrain_info_from_rasmap(rasmap_path)

        if not self._terrain_cache:
            messages.addErrorMessage("No terrains found or .rasmap file could not be read. Aborting.")
            return

        terrains_to_load = []
        if import_all:
            terrains_to_load = list(self._terrain_cache.keys())
            messages.addMessage("Import All selected. Loading all available terrains...")
        elif selected_terrains:
            terrains_to_load = selected_terrains
            messages.addMessage(f"Loading selected terrains: {', '.join(terrains_to_load)}")
        else:
            messages.addErrorMessage("No terrains selected for loading.")
            return
            
        try:
            aprx = arcpy.mp.ArcGISProject("CURRENT")
            # Ensure there is an active map
            if not aprx.activeMap:
                 messages.addErrorMessage("No active map found. Please open a map view and try again.")
                 return
            map = aprx.activeMap
        except Exception as e:
            messages.addErrorMessage(f"Could not access the current ArcGIS Pro project or map: {e}")
            return
            
        layers_added = 0
        for terrain_name in terrains_to_load:
            hdf_path_str = self._terrain_cache.get(terrain_name)
            if not hdf_path_str:
                messages.addWarningMessage(f"Could not find path for terrain '{terrain_name}'. Skipping.")
                continue
            
            p = Path(hdf_path_str)
            vrt_path = str(p.with_suffix('.vrt'))
            
            if os.path.exists(vrt_path):
                try:
                    map.addDataFromPath(vrt_path)
                    messages.addMessage(f"Successfully added terrain layer: {terrain_name}")
                    layers_added += 1
                except Exception as e:
                    messages.addWarningMessage(f"Failed to add layer for '{terrain_name}' from path {vrt_path}: {e}")
            else:
                messages.addWarningMessage(f"Associated VRT file not found for terrain '{terrain_name}'. Expected at: {vrt_path}")
        
        messages.addMessage(f"\nProcessing complete. Added {layers_added} terrain layer(s).")
        return
    
    def getHelp(self, tool_name):
        """Return help documentation URL for the tool."""
        help_file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
                                "Doc", "RASCommander_Help.html")
        if os.path.exists(help_file):
            return f"file:///{help_file.replace(os.sep, '/')}#load-hec-ras-terrain"
        return None
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_organize_ras_project.py
==================================================
# -*- coding: utf-8 -*-
"""
OrganizeRASProject.py

Master tool for organizing all HEC-RAS data from HDF files into a structured geodatabase.
This tool extracts all available geometry and results data in a single operation.
"""

import arcpy
import os
import h5py
import numpy as np

# Import helper functions from utils
from rc_utils import (
    get_ras_projection_wkt,
    setup_geodatabase_output,
    get_unique_fc_name,
    add_feature_class_metadata,
    extract_project_and_plan_info,
    create_geodatabase_from_hdf,
    get_feature_dataset_name,
    get_feature_class_name
)

# Import the individual tool classes
from rc_load_hecras_1d_geometry import LoadHECRAS1DGeometry
from rc_load_hecras_2d_geometry import LoadHECRAS2DGeometry
from rc_load_hecras_2d_results import LoadHECRAS2DResults


class OrganizeRASProject(object):
    """
    Organizes all HEC-RAS data into a structured geodatabase.
    """
    def __init__(self):
        self.label = "Organize HEC-RAS Project"
        self.description = """Extracts all geometry and results from HEC-RAS files into an organized geodatabase.
        
        This tool automatically:
        • Creates a well-organized geodatabase structure
        • Extracts all available 1D geometry elements
        • Extracts all available 2D geometry elements
        • Extracts pipe network data if present
        • Extracts results data if available
        
        The geodatabase will be organized with feature datasets named by project and plan:
        • {ProjectName}_Plan{XX} - Contains all geometry and results for each plan
        
        Each plan's feature dataset will contain:
        • 1D Geometry - Cross sections, river centerlines, bank lines, structures
        • 2D Geometry - Breaklines, boundary conditions, mesh elements
        • Pipe Networks - Storm/sewer pipe networks (if present)
        • Results - Maximum WSE, velocity, and other results (if available)
        
        Note: This operation may take several minutes for large models."""
        self.canRunInBackground = False

    def getParameterInfo(self):
        params = [
            # Input files
            arcpy.Parameter(displayName="HEC-RAS Project Directory or HDF File", name="input_path", 
                          datatype=["DEFolder", "DEFile"], 
                          parameterType="Required", direction="Input"),
            
            # Output geodatabase
            arcpy.Parameter(displayName="Output Geodatabase", name="output_gdb", datatype="DEWorkspace", 
                          parameterType="Required", direction="Output"),
            
            # CRS override
            arcpy.Parameter(displayName="Override CRS (Optional)", name="override_crs", datatype="GPSpatialReference", 
                          parameterType="Optional", direction="Input"),
            
            # Options
            arcpy.Parameter(displayName="Include Mesh Cell Polygons", name="include_cell_polygons", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input"),
            arcpy.Parameter(displayName="Extract All Available Results", name="extract_all_results", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input")
        ]
        
        # Configure parameters
        params[0].description = """Select a HEC-RAS project directory to process all plan files (p*.hdf), 
        or select a single HDF file to process."""
        
        params[1].description = "Output geodatabase that will contain all extracted data in an organized structure."
        
        params[2].description = """Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files."""
        
        params[3].value = False
        params[3].description = "Include mesh cell polygons (can be time-consuming for large meshes)."
        
        params[4].value = True
        params[4].description = "Extract all available result variables (WSE, velocity, depth, etc.)."
        
        return params

    def isLicensed(self):
        """Set whether tool is licensed to execute."""
        return True

    def updateParameters(self, parameters):
        """Modify the values and properties of parameters before internal validation."""
        # Set default output geodatabase name based on input
        if parameters[0].value and not parameters[1].altered:
            input_path = parameters[0].valueAsText
            
            if os.path.isdir(input_path):
                # For directory, use directory name
                input_name = os.path.basename(input_path.rstrip(os.sep))
                default_gdb = os.path.join(input_path, f"{input_name}_Organized.gdb")
            else:
                # For single file, use file name
                input_name = os.path.splitext(os.path.basename(input_path))[0]
                default_gdb = os.path.join(os.path.dirname(input_path), f"{input_name}_Organized.gdb")
            
            parameters[1].value = default_gdb
        return
    
    def updateMessages(self, parameters):
        """Modify messages created by internal validation."""
        # Warn about mesh polygons
        if parameters[3].value:
            parameters[3].setWarningMessage(
                "Creating cell polygons can be time-consuming for large meshes (>100,000 cells)."
            )
        return

    def _check_hdf_contents(self, hdf_file):
        """Check what data is available in the HDF file."""
        contents = {
            'has_1d': False,
            'has_2d': False,
            'has_pipes': False,
            'has_results': False,
            '1d_elements': [],
            '2d_elements': [],
            'result_profiles': []
        }
        
        # Check for 1D geometry
        if "Geometry/Cross Sections" in hdf_file:
            # Verify that it has actual data
            if "Geometry/Cross Sections/Attributes" in hdf_file:
                contents['has_1d'] = True
                contents['1d_elements'].append("Cross Sections")
        if "Geometry/River Centerlines" in hdf_file:
            contents['has_1d'] = True
            contents['1d_elements'].append("River Centerlines")
        if "Geometry/Bank Lines" in hdf_file:
            contents['has_1d'] = True
            contents['1d_elements'].append("Bank Lines")
        if "Geometry/Structures" in hdf_file:
            # Verify that it has actual data
            if "Geometry/Structures/Attributes" in hdf_file:
                contents['has_1d'] = True
                contents['1d_elements'].append("1D Structures")
        
        # Check for 2D geometry
        if "Geometry/2D Flow Areas" in hdf_file:
            contents['has_2d'] = True
            contents['2d_elements'].append("Mesh Area Perimeters")
            contents['2d_elements'].append("Mesh Cell Centers")
            contents['2d_elements'].append("Mesh Cell Faces")
        if "Geometry/2D Flow Area Break Lines" in hdf_file:
            contents['2d_elements'].append("2D Breaklines")
        if "Geometry/Boundary Condition Lines" in hdf_file:
            contents['2d_elements'].append("2D Boundary Condition Lines")
        
        # Check for pipe networks
        if "Geometry/Pipe Conduits" in hdf_file:
            contents['has_pipes'] = True
        
        # Check for results
        if "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/2D Flow Areas" in hdf_file:
            contents['has_results'] = True
            # Get available output profiles
            results_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/2D Flow Areas"
            for flow_area in hdf_file[results_path]:
                if "Maximum Values" in hdf_file[f"{results_path}/{flow_area}"]:
                    max_vals = hdf_file[f"{results_path}/{flow_area}/Maximum Values"]
                    contents['result_profiles'].extend(list(max_vals.keys()))
                    break
        
        return contents

    def _load_geodatabase_to_map(self, gdb_path, messages):
        """Load all feature classes from the geodatabase into the current map, grouped by plan."""
        try:
            # Get the current project and map
            aprx = arcpy.mp.ArcGISProject("CURRENT")
            map_obj = aprx.activeMap
            
            if not map_obj:
                messages.addWarning("No active map found. Feature classes were created but not added to the map.")
                return
            
            # Collect all feature classes and rasters first
            all_layers = []
            
            # Set workspace to the geodatabase
            arcpy.env.workspace = gdb_path
            
            # Collect all feature classes
            for dirpath, dirnames, filenames in arcpy.da.Walk(gdb_path, datatype="FeatureClass"):
                for filename in filenames:
                    fc_path = os.path.join(dirpath, filename)
                    all_layers.append((filename, fc_path, "FeatureClass"))
            
            # Collect all rasters
            for dirpath, dirnames, filenames in arcpy.da.Walk(gdb_path, datatype="RasterDataset"):
                for filename in filenames:
                    raster_path = os.path.join(dirpath, filename)
                    all_layers.append((filename, raster_path, "Raster"))
            
            # Group layers by plan
            plan_groups = {}
            for layer_name, layer_path, layer_type in all_layers:
                # Extract plan info from layer name (e.g., "ProjectName_Plan_03_LayerType")
                if "_Plan_" in layer_name:
                    parts = layer_name.split("_Plan_")
                    if len(parts) >= 2:
                        project_name = parts[0]
                        # Extract plan number (should be next 2 characters after Plan_)
                        remaining = parts[1]
                        if len(remaining) >= 2:
                            plan_number = remaining[:2]
                            plan_key = "{}_Plan_{}".format(project_name, plan_number)
                            
                            if plan_key not in plan_groups:
                                plan_groups[plan_key] = []
                            plan_groups[plan_key].append((layer_name, layer_path, layer_type))
                else:
                    # Layers without plan info go to a general group
                    if "General" not in plan_groups:
                        plan_groups["General"] = []
                    plan_groups["General"].append((layer_name, layer_path, layer_type))
            
            # Sort plans and layers within each plan
            sorted_plans = sorted(plan_groups.keys())
            
            # Add layers to map grouped by plan
            feature_classes_added = []
            messages.addMessage("\nAdding layers to map grouped by plan:")
            
            for plan_key in sorted_plans:
                # Create a group layer for this plan
                group_name = plan_key if plan_key != "General" else "Other Layers"
                
                try:
                    # Create the group layer first
                    group_layer = map_obj.createGroupLayer(group_name)
                    
                    # Sort layers within this plan alphabetically
                    plan_layers = sorted(plan_groups[plan_key], key=lambda x: x[0].lower())
                    
                    # Add layers to the group
                    layers_added_to_group = 0
                    layers_to_remove = []  # Track layers to remove after adding to group
                    
                    for layer_name, layer_path, layer_type in plan_layers:
                        try:
                            # First add the layer to the map
                            layer = map_obj.addDataFromPath(layer_path)
                            if layer:
                                # Then add it to the group layer
                                map_obj.addLayerToGroup(group_layer, layer, "AUTO_ARRANGE")
                                
                                # Track the original layer for removal
                                layers_to_remove.append(layer)
                                
                                feature_classes_added.append(layer_name)
                                layers_added_to_group += 1
                                messages.addMessage("  Added {} to {}".format(layer_name, group_name))
                        except Exception as e:
                            messages.addWarning("  Could not add {} to group: {}".format(layer_name, str(e)))
                    
                    # Remove the original layers that were added to the group
                    for layer in layers_to_remove:
                        try:
                            map_obj.removeLayer(layer)
                        except:
                            pass  # Ignore errors when removing
                    
                    if layers_added_to_group > 0:
                        messages.addMessage("Created group layer: {} with {} layers".format(group_name, layers_added_to_group))
                    else:
                        # Remove empty group layer
                        try:
                            map_obj.removeLayer(group_layer)
                        except:
                            pass
                        
                except Exception as e:
                    messages.addWarning("Could not create group layer {}: {}".format(group_name, str(e)))
                    # Fall back to adding layers to the map directly
                    plan_layers = sorted(plan_groups[plan_key], key=lambda x: x[0].lower())
                    for layer_name, layer_path, layer_type in plan_layers:
                        try:
                            layer = map_obj.addDataFromPath(layer_path)
                            if layer:
                                feature_classes_added.append(layer_name)
                                messages.addMessage("  Added {} to map (ungrouped)".format(layer_name))
                        except Exception as e:
                            messages.addWarning("  Could not add {}: {}".format(layer_name, str(e)))
            
            if feature_classes_added:
                messages.addMessage("\nSuccessfully added {} layers to the map in {} groups.".format(len(feature_classes_added), len(plan_groups)))
                
                # Try to save the project
                try:
                    aprx.save()
                    messages.addMessage("Project saved.")
                except:
                    pass  # Saving might fail in some contexts
            else:
                messages.addWarning("No feature classes were added to the map.")
                
        except Exception as e:
            messages.addWarning("Error loading geodatabase to map: {}".format(str(e)))
            messages.addWarning("Feature classes were created successfully but could not be added to the map.")

    def execute(self, parameters, messages):
        """Execute the tool."""
        input_path = parameters[0].valueAsText
        output_gdb = parameters[1].valueAsText
        override_crs = parameters[2].value
        include_cell_polygons = parameters[3].value
        extract_all_results = parameters[4].value
        
        # Determine if input is directory or file
        hdf_files = []
        
        if os.path.isdir(input_path):
            # Find all plan files
            import glob
            pattern = os.path.join(input_path, "*.p[0-9][0-9].hdf")
            hdf_files = sorted(glob.glob(pattern))
            
            if not hdf_files:
                # Try geometry files
                pattern = os.path.join(input_path, "*.g[0-9][0-9].hdf")
                hdf_files = sorted(glob.glob(pattern))
        else:
            # Single file
            hdf_files = [input_path]
        
        if not hdf_files:
            messages.addErrorMessage("No HDF files found in the specified location.")
            return
        
        messages.addMessage(f"Found {len(hdf_files)} HDF file(s) to process")
        
        # Create output geodatabase
        gdb_folder = os.path.dirname(output_gdb)
        gdb_name = os.path.basename(output_gdb)
        
        if not arcpy.Exists(output_gdb):
            arcpy.CreateFileGDB_management(gdb_folder, gdb_name)
            messages.addMessage(f"Created geodatabase: {output_gdb}")
        
        # Process each HDF file
        for i, hdf_path in enumerate(hdf_files, 1):
            messages.addMessage(f"\n{'='*60}")
            messages.addMessage(f"Processing file {i}/{len(hdf_files)}: {os.path.basename(hdf_path)}")
            messages.addMessage(f"{'='*60}")
            
            self._process_single_hdf(hdf_path, output_gdb, override_crs, 
                                   include_cell_polygons, extract_all_results, messages)
        
        messages.addMessage(f"\n{'='*60}")
        messages.addMessage(f"Processing complete! All plans organized in:")
        messages.addMessage(f"  {output_gdb}")
        
        # Load the geodatabase into the map
        messages.addMessage("\nAdding results to map...")
        self._load_geodatabase_to_map(output_gdb, messages)
    
    def _process_single_hdf(self, hdf_path, output_gdb, override_crs, 
                          include_cell_polygons, extract_all_results, messages):
        """Process a single HDF file."""
        # Extract project and plan info
        project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
        
        # Get projection
        proj_wkt = get_ras_projection_wkt(hdf_path)
        sr = None
        if proj_wkt:
            sr = arcpy.SpatialReference()
            sr.loadFromString(proj_wkt)
            messages.addMessage(f"CRS '{sr.name}' found in HEC-RAS project files.")
        elif override_crs:
            sr = override_crs
            messages.addMessage(f"Using user-defined override CRS: {sr.name}")
        else:
            messages.addErrorMessage("CRS could not be determined. Please use the Override CRS parameter.")
            raise arcpy.ExecuteError
        
        # Create feature dataset for this plan
        feature_dataset_name = get_feature_dataset_name(hdf_path)
        fd_path = setup_geodatabase_output(output_gdb, feature_dataset_name, sr, messages)
        
        # Check HDF contents
        messages.addMessage("\nAnalyzing HDF file contents...")
        with h5py.File(hdf_path, 'r') as hdf_file:
            contents = self._check_hdf_contents(hdf_file)
        
        # Report what was found
        if contents['has_1d']:
            messages.addMessage(f"Found 1D geometry: {', '.join(contents['1d_elements'])}")
        if contents['has_2d']:
            messages.addMessage(f"Found 2D geometry: {', '.join(contents['2d_elements'])}")
        if contents['has_pipes']:
            messages.addMessage("Found pipe network data")
        if contents['has_results']:
            messages.addMessage(f"Found results data with {len(set(contents['result_profiles']))} output variables")
        
        # Create mock parameters class for tool execution
        class MockParam:
            def __init__(self, value):
                self.value = value
                self.valueAsText = str(value) if value else None
                self.values = value if isinstance(value, list) else None
        
        # Process 1D Geometry
        if contents['has_1d']:
            messages.addMessage("\n--- Processing 1D Geometry ---")
            tool_1d = LoadHECRAS1DGeometry()
            
            # Use the plan's feature dataset
            fd_1d = fd_path
            
            # Create parameters for 1D tool
            # Note: We pass None for geodatabase parameter to prevent the individual tools
            # from overriding our carefully constructed feature class names that include
            # the project name and plan number
            params_1d = [
                hdf_path,  # input HDF
                override_crs,  # override CRS
                contents['1d_elements'],  # elements to load
                None, None, None, None, None,  # output paths (will be set individually)
                None,  # geodatabase (None to prevent tools from overriding our paths)
                False  # create_gdb (False because we already created it)
            ]
            
            mock_params = [MockParam(p) for p in params_1d]
            
            # Set output paths with full naming convention for multi-plan processing
            if "Cross Sections" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_CrossSections"
                mock_params[3].valueAsText = os.path.join(fd_path, fc_name)
            if "River Centerlines" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_RiverCenterlines"
                mock_params[4].valueAsText = os.path.join(fd_path, fc_name)
            if "Bank Lines" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_BankLines"
                mock_params[5].valueAsText = os.path.join(fd_path, fc_name)
            if "Edge Lines" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_EdgeLines"
                mock_params[6].valueAsText = os.path.join(fd_path, fc_name)
            if "1D Structures" in contents['1d_elements'] or "Hydraulic Structures" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_Structures1D"
                mock_params[7].valueAsText = os.path.join(fd_path, fc_name)
            
            # Execute 1D tool
            try:
                tool_1d.execute(mock_params, messages)
            except Exception as e:
                messages.addWarning(f"Error processing 1D geometry: {e}")
                messages.addWarning("Continuing with remaining data...")
        
        # Process 2D Geometry
        if contents['has_2d']:
            messages.addMessage("\n--- Processing 2D Geometry ---")
            tool_2d = LoadHECRAS2DGeometry()
            
            # Determine which elements to load
            elements_2d = contents['2d_elements'][:]
            if include_cell_polygons:
                elements_2d.append("Mesh Cells (Polygons)")
            
            # Create parameters for 2D tool
            params_2d = [
                hdf_path,  # input HDF
                override_crs,  # override CRS
                elements_2d,  # elements to load
                None, None, None, None, None, None, None, None, None,  # output paths
                None,  # geodatabase (None to prevent tools from overriding our paths)
                False  # create_gdb
            ]
            
            mock_params_2d = [MockParam(p) for p in params_2d]
            
            # Set output paths with full naming convention for multi-plan processing
            # Map indices to element names
            element_indices = {
                3: ("2D Breaklines", "Breaklines2D"),
                4: ("2D Boundary Condition Lines", "BCLines2D"),
                5: ("Mesh Area Perimeters", "MeshPerimeters"),
                6: ("Mesh Cell Centers", "MeshCellCenters"),
                7: ("Mesh Cell Faces", "MeshCellFaces"),
                8: ("Mesh Cells (Polygons)", "MeshCellPolygons"),
                9: ("Pipe Conduits", "PipeConduits"),
                10: ("Pipe Nodes", "PipeNodes"),
                11: ("Pipe Networks", "PipeNetworks")
            }
            
            for idx, (element_name, fc_base) in element_indices.items():
                if element_name in elements_2d:
                    fc_name = f"{project_name}_Plan_{plan_number}_{fc_base}"
                    mock_params_2d[idx].valueAsText = os.path.join(fd_path, fc_name)
            
            # Execute 2D tool
            try:
                tool_2d.execute(mock_params_2d, messages)
            except Exception as e:
                messages.addWarning(f"Error processing 2D geometry: {e}")
                messages.addWarning("Continuing with remaining data...")
        
        # Process Results
        if contents['has_results'] and extract_all_results:
            messages.addMessage("\n--- Processing Results ---")
            tool_results = LoadHECRAS2DResults()
            
            # Create parameters for results tool
            params_results = [
                hdf_path,  # input HDF
                override_crs,  # override CRS
                ["Max WSE at Cell Centers", "Max Vel at Cell Faces"],  # results elements to load
                None,  # output max wse
                None,  # output max vel
                None,  # geodatabase (None to prevent tools from overriding our paths)
                False  # create_gdb
            ]
            
            mock_params_results = [MockParam(p) for p in params_results]
            
            # Set result output paths with full naming convention
            fc_name = f"{project_name}_Plan_{plan_number}_MaxWSE"
            mock_params_results[3].valueAsText = os.path.join(fd_path, fc_name)
            
            fc_name = f"{project_name}_Plan_{plan_number}_MaxVelocity"
            mock_params_results[4].valueAsText = os.path.join(fd_path, fc_name)
            
            # Execute results tool
            try:
                tool_results.execute(mock_params_results, messages)
            except Exception as e:
                messages.addWarning(f"Error processing results: {e}")
                messages.addWarning("Continuing with remaining data...")
        
        messages.addMessage(f"\nCompleted processing: {os.path.basename(hdf_path)}")
        return
    
    def getHelp(self, tool_name):
        """Return help documentation URL for the tool."""
        help_file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
                                "Doc", "RASCommander_Help.html")
        if os.path.exists(help_file):
            return f"file:///{help_file.replace(os.sep, '/')}#organize-hec-ras-project"
        return None
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_organize_ras_project.py.backup
==================================================
# -*- coding: utf-8 -*-
"""
OrganizeRASProject.py

Master tool for organizing all HEC-RAS data from HDF files into a structured geodatabase.
This tool extracts all available geometry and results data in a single operation.
"""

import arcpy
import os
import h5py
import numpy as np

# Import helper functions from utils
from rc_utils import (
    get_ras_projection_wkt,
    setup_geodatabase_output,
    get_unique_fc_name,
    add_feature_class_metadata,
    extract_project_and_plan_info,
    create_geodatabase_from_hdf,
    get_feature_dataset_name,
    get_feature_class_name
)

# Import the individual tool classes
from rc_load_hecras_1d_geometry import LoadHECRAS1DGeometry
from rc_load_hecras_2d_geometry import LoadHECRAS2DGeometry
from rc_load_hecras_2d_results import LoadHECRAS2DResults


class OrganizeRASProject(object):
    """
    Organizes all HEC-RAS data into a structured geodatabase.
    """
    def __init__(self):
        self.label = "Organize HEC-RAS Project"
        self.description = """Extracts all geometry and results from HEC-RAS files into an organized geodatabase.
        
        This tool automatically:
        • Creates a well-organized geodatabase structure
        • Extracts all available 1D geometry elements
        • Extracts all available 2D geometry elements
        • Extracts pipe network data if present
        • Extracts results data if available
        
        The geodatabase will be organized with feature datasets named by project and plan:
        • {ProjectName}_Plan{XX} - Contains all geometry and results for each plan
        
        Each plan's feature dataset will contain:
        • 1D Geometry - Cross sections, river centerlines, bank lines, structures
        • 2D Geometry - Breaklines, boundary conditions, mesh elements
        • Pipe Networks - Storm/sewer pipe networks (if present)
        • Results - Maximum WSE, velocity, and other results (if available)
        
        Note: This operation may take several minutes for large models."""
        self.canRunInBackground = False

    def getParameterInfo(self):
        params = [
            # Input files
            arcpy.Parameter(displayName="HEC-RAS Project Directory or HDF File", name="input_path", 
                          datatype=["DEFolder", "DEFile"], 
                          parameterType="Required", direction="Input"),
            
            # Output geodatabase
            arcpy.Parameter(displayName="Output Geodatabase", name="output_gdb", datatype="DEWorkspace", 
                          parameterType="Required", direction="Output"),
            
            # CRS override
            arcpy.Parameter(displayName="Override CRS (Optional)", name="override_crs", datatype="GPSpatialReference", 
                          parameterType="Optional", direction="Input"),
            
            # Options
            arcpy.Parameter(displayName="Include Mesh Cell Polygons", name="include_cell_polygons", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input"),
            arcpy.Parameter(displayName="Extract All Available Results", name="extract_all_results", datatype="GPBoolean", 
                          parameterType="Optional", direction="Input")
        ]
        
        # Configure parameters
        params[0].description = """Select a HEC-RAS project directory to process all plan files (p*.hdf), 
        or select a single HDF file to process."""
        
        params[1].description = "Output geodatabase that will contain all extracted data in an organized structure."
        
        params[2].description = """Specify a coordinate reference system if it cannot be determined from the HEC-RAS project files."""
        
        params[3].value = False
        params[3].description = "Include mesh cell polygons (can be time-consuming for large meshes)."
        
        params[4].value = True
        params[4].description = "Extract all available result variables (WSE, velocity, depth, etc.)."
        
        return params

    def isLicensed(self):
        """Set whether tool is licensed to execute."""
        return True

    def updateParameters(self, parameters):
        """Modify the values and properties of parameters before internal validation."""
        # Set default output geodatabase name based on input
        if parameters[0].value and not parameters[1].altered:
            input_path = parameters[0].valueAsText
            
            if os.path.isdir(input_path):
                # For directory, use directory name
                input_name = os.path.basename(input_path.rstrip(os.sep))
                default_gdb = os.path.join(input_path, f"{input_name}_Organized.gdb")
            else:
                # For single file, use file name
                input_name = os.path.splitext(os.path.basename(input_path))[0]
                default_gdb = os.path.join(os.path.dirname(input_path), f"{input_name}_Organized.gdb")
            
            parameters[1].value = default_gdb
        return
    
    def updateMessages(self, parameters):
        """Modify messages created by internal validation."""
        # Warn about mesh polygons
        if parameters[3].value:
            parameters[3].setWarningMessage(
                "Creating cell polygons can be time-consuming for large meshes (>100,000 cells)."
            )
        return

    def _check_hdf_contents(self, hdf_file):
        """Check what data is available in the HDF file."""
        contents = {
            'has_1d': False,
            'has_2d': False,
            'has_pipes': False,
            'has_results': False,
            '1d_elements': [],
            '2d_elements': [],
            'result_profiles': []
        }
        
        # Check for 1D geometry
        if "Geometry/Cross Sections" in hdf_file:
            # Verify that it has actual data
            if "Geometry/Cross Sections/Attributes" in hdf_file:
                contents['has_1d'] = True
                contents['1d_elements'].append("Cross Sections")
        if "Geometry/River Centerlines" in hdf_file:
            contents['has_1d'] = True
            contents['1d_elements'].append("River Centerlines")
        if "Geometry/Bank Lines" in hdf_file:
            contents['has_1d'] = True
            contents['1d_elements'].append("Bank Lines")
        if "Geometry/Structures" in hdf_file:
            # Verify that it has actual data
            if "Geometry/Structures/Attributes" in hdf_file:
                contents['has_1d'] = True
                contents['1d_elements'].append("1D Structures")
        
        # Check for 2D geometry
        if "Geometry/2D Flow Areas" in hdf_file:
            contents['has_2d'] = True
            contents['2d_elements'].append("Mesh Area Perimeters")
            contents['2d_elements'].append("Mesh Cell Centers")
            contents['2d_elements'].append("Mesh Cell Faces")
        if "Geometry/2D Flow Area Break Lines" in hdf_file:
            contents['2d_elements'].append("2D Breaklines")
        if "Geometry/Boundary Condition Lines" in hdf_file:
            contents['2d_elements'].append("2D Boundary Condition Lines")
        
        # Check for pipe networks
        if "Geometry/Pipe Conduits" in hdf_file:
            contents['has_pipes'] = True
        
        # Check for results
        if "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/2D Flow Areas" in hdf_file:
            contents['has_results'] = True
            # Get available output profiles
            results_path = "Results/Unsteady/Output/Output Blocks/Base Output/Unsteady Time Series/2D Flow Areas"
            for flow_area in hdf_file[results_path]:
                if "Maximum Values" in hdf_file[f"{results_path}/{flow_area}"]:
                    max_vals = hdf_file[f"{results_path}/{flow_area}/Maximum Values"]
                    contents['result_profiles'].extend(list(max_vals.keys()))
                    break
        
        return contents

    def _load_geodatabase_to_map(self, gdb_path, messages):
        """Load all feature classes from the geodatabase into the current map, grouped by plan."""
        try:
            # Get the current project and map
            aprx = arcpy.mp.ArcGISProject("CURRENT")
            map_obj = aprx.activeMap
            
            if not map_obj:
                messages.addWarning("No active map found. Feature classes were created but not added to the map.")
                return
            
            # Collect all feature classes and rasters first
            all_layers = []
            
            # Set workspace to the geodatabase
            arcpy.env.workspace = gdb_path
            
            # Collect all feature classes
            for dirpath, dirnames, filenames in arcpy.da.Walk(gdb_path, datatype="FeatureClass"):
                for filename in filenames:
                    fc_path = os.path.join(dirpath, filename)
                    all_layers.append((filename, fc_path, "FeatureClass"))
            
            # Collect all rasters
            for dirpath, dirnames, filenames in arcpy.da.Walk(gdb_path, datatype="RasterDataset"):
                for filename in filenames:
                    raster_path = os.path.join(dirpath, filename)
                    all_layers.append((filename, raster_path, "Raster"))
            
            # Group layers by plan
            plan_groups = {}
            for layer_name, layer_path, layer_type in all_layers:
                # Extract plan info from layer name (e.g., "ProjectName_Plan_03_LayerType")
                if "_Plan_" in layer_name:
                    parts = layer_name.split("_Plan_")
                    if len(parts) >= 2:
                        project_name = parts[0]
                        # Extract plan number (should be next 2 characters after Plan_)
                        remaining = parts[1]
                        if len(remaining) >= 2:
                            plan_number = remaining[:2]
                            plan_key = f"{project_name}_Plan_{plan_number}"
                            
                            if plan_key not in plan_groups:
                                plan_groups[plan_key] = []
                            plan_groups[plan_key].append((layer_name, layer_path, layer_type))
                else:
                    # Layers without plan info go to a general group
                    if "General" not in plan_groups:
                        plan_groups["General"] = []
                    plan_groups["General"].append((layer_name, layer_path, layer_type))
            
            # Sort plans and layers within each plan
            sorted_plans = sorted(plan_groups.keys())
            
            # Add layers to map grouped by plan
            feature_classes_added = []
            messages.addMessage("\nAdding layers to map grouped by plan:")
            
            for plan_key in sorted_plans:
                # Create a group layer for this plan
                group_name = plan_key if plan_key != "General" else "Other Layers"
                
                try:
                    # Create the group layer first
                    group_layer = map_obj.createGroupLayer(group_name)
                    
                    # Sort layers within this plan alphabetically
                    plan_layers = sorted(plan_groups[plan_key], key=lambda x: x[0].lower())
                    
                    # Add layers directly to the group layer
                    layers_added_to_group = 0
                    for layer_name, layer_path, layer_type in plan_layers:
                        try:
                            # Add the layer directly to the group layer
                            layer = group_layer.addDataFromPath(layer_path)
                            if layer:
                                feature_classes_added.append(layer_name)
                                layers_added_to_group += 1
                                messages.addMessage(f"  Added {layer_name} to {group_name}")
                        except Exception as e:
                            messages.addWarning(f"  Could not add {layer_name} to group: {e}")
                    
                    if layers_added_to_group > 0:
                        messages.addMessage(f"Created group layer: {group_name} with {layers_added_to_group} layers")
                    else:
                        # Remove empty group layer
                        map_obj.removeLayer(group_layer)
                        
                except Exception as e:
                    messages.addWarning(f"Could not create group layer {group_name}: {e}")
                    # Fall back to adding layers to the map directly
                    plan_layers = sorted(plan_groups[plan_key], key=lambda x: x[0].lower())
                    for layer_name, layer_path, layer_type in plan_layers:
                        try:
                            layer = map_obj.addDataFromPath(layer_path)
                            if layer:
                                feature_classes_added.append(layer_name)
                                messages.addMessage(f"  Added {layer_name} to map (ungrouped)")
                        except Exception as e:
                            messages.addWarning(f"  Could not add {layer_name}: {e}")
            
            if feature_classes_added:
                messages.addMessage(f"\nSuccessfully added {len(feature_classes_added)} layers to the map in {len(plan_groups)} groups.")
                
                # Try to save the project
                try:
                    aprx.save()
                    messages.addMessage("Project saved.")
                except:
                    pass  # Saving might fail in some contexts
            else:
                messages.addWarning("No feature classes were added to the map.")
                
        except Exception as e:
            messages.addWarning(f"Error loading geodatabase to map: {e}")
            messages.addWarning("Feature classes were created successfully but could not be added to the map.")

    def execute(self, parameters, messages):
        """Execute the tool."""
        input_path = parameters[0].valueAsText
        output_gdb = parameters[1].valueAsText
        override_crs = parameters[2].value
        include_cell_polygons = parameters[3].value
        extract_all_results = parameters[4].value
        
        # Determine if input is directory or file
        hdf_files = []
        
        if os.path.isdir(input_path):
            # Find all plan files
            import glob
            pattern = os.path.join(input_path, "*.p[0-9][0-9].hdf")
            hdf_files = sorted(glob.glob(pattern))
            
            if not hdf_files:
                # Try geometry files
                pattern = os.path.join(input_path, "*.g[0-9][0-9].hdf")
                hdf_files = sorted(glob.glob(pattern))
        else:
            # Single file
            hdf_files = [input_path]
        
        if not hdf_files:
            messages.addErrorMessage("No HDF files found in the specified location.")
            return
        
        messages.addMessage(f"Found {len(hdf_files)} HDF file(s) to process")
        
        # Create output geodatabase
        gdb_folder = os.path.dirname(output_gdb)
        gdb_name = os.path.basename(output_gdb)
        
        if not arcpy.Exists(output_gdb):
            arcpy.CreateFileGDB_management(gdb_folder, gdb_name)
            messages.addMessage(f"Created geodatabase: {output_gdb}")
        
        # Process each HDF file
        for i, hdf_path in enumerate(hdf_files, 1):
            messages.addMessage(f"\n{'='*60}")
            messages.addMessage(f"Processing file {i}/{len(hdf_files)}: {os.path.basename(hdf_path)}")
            messages.addMessage(f"{'='*60}")
            
            self._process_single_hdf(hdf_path, output_gdb, override_crs, 
                                   include_cell_polygons, extract_all_results, messages)
        
        messages.addMessage(f"\n{'='*60}")
        messages.addMessage(f"Processing complete! All plans organized in:")
        messages.addMessage(f"  {output_gdb}")
        
        # Load the geodatabase into the map
        messages.addMessage("\nAdding results to map...")
        self._load_geodatabase_to_map(output_gdb, messages)
    
    def _process_single_hdf(self, hdf_path, output_gdb, override_crs, 
                          include_cell_polygons, extract_all_results, messages):
        """Process a single HDF file."""
        # Extract project and plan info
        project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
        
        # Get projection
        proj_wkt = get_ras_projection_wkt(hdf_path)
        sr = None
        if proj_wkt:
            sr = arcpy.SpatialReference()
            sr.loadFromString(proj_wkt)
            messages.addMessage(f"CRS '{sr.name}' found in HEC-RAS project files.")
        elif override_crs:
            sr = override_crs
            messages.addMessage(f"Using user-defined override CRS: {sr.name}")
        else:
            messages.addErrorMessage("CRS could not be determined. Please use the Override CRS parameter.")
            raise arcpy.ExecuteError
        
        # Create feature dataset for this plan
        feature_dataset_name = get_feature_dataset_name(hdf_path)
        fd_path = setup_geodatabase_output(output_gdb, feature_dataset_name, sr, messages)
        
        # Check HDF contents
        messages.addMessage("\nAnalyzing HDF file contents...")
        with h5py.File(hdf_path, 'r') as hdf_file:
            contents = self._check_hdf_contents(hdf_file)
        
        # Report what was found
        if contents['has_1d']:
            messages.addMessage(f"Found 1D geometry: {', '.join(contents['1d_elements'])}")
        if contents['has_2d']:
            messages.addMessage(f"Found 2D geometry: {', '.join(contents['2d_elements'])}")
        if contents['has_pipes']:
            messages.addMessage("Found pipe network data")
        if contents['has_results']:
            messages.addMessage(f"Found results data with {len(set(contents['result_profiles']))} output variables")
        
        # Create mock parameters class for tool execution
        class MockParam:
            def __init__(self, value):
                self.value = value
                self.valueAsText = str(value) if value else None
                self.values = value if isinstance(value, list) else None
        
        # Process 1D Geometry
        if contents['has_1d']:
            messages.addMessage("\n--- Processing 1D Geometry ---")
            tool_1d = LoadHECRAS1DGeometry()
            
            # Use the plan's feature dataset
            fd_1d = fd_path
            
            # Create parameters for 1D tool
            # Note: We pass None for geodatabase parameter to prevent the individual tools
            # from overriding our carefully constructed feature class names that include
            # the project name and plan number
            params_1d = [
                hdf_path,  # input HDF
                override_crs,  # override CRS
                contents['1d_elements'],  # elements to load
                None, None, None, None, None,  # output paths (will be set individually)
                None,  # geodatabase (None to prevent tools from overriding our paths)
                False  # create_gdb (False because we already created it)
            ]
            
            mock_params = [MockParam(p) for p in params_1d]
            
            # Set output paths with full naming convention for multi-plan processing
            if "Cross Sections" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_CrossSections"
                mock_params[3].valueAsText = os.path.join(fd_path, fc_name)
            if "River Centerlines" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_RiverCenterlines"
                mock_params[4].valueAsText = os.path.join(fd_path, fc_name)
            if "Bank Lines" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_BankLines"
                mock_params[5].valueAsText = os.path.join(fd_path, fc_name)
            if "Edge Lines" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_EdgeLines"
                mock_params[6].valueAsText = os.path.join(fd_path, fc_name)
            if "1D Structures" in contents['1d_elements'] or "Hydraulic Structures" in contents['1d_elements']:
                fc_name = f"{project_name}_Plan_{plan_number}_Structures1D"
                mock_params[7].valueAsText = os.path.join(fd_path, fc_name)
            
            # Execute 1D tool
            try:
                tool_1d.execute(mock_params, messages)
            except Exception as e:
                messages.addWarning(f"Error processing 1D geometry: {e}")
                messages.addWarning("Continuing with remaining data...")
        
        # Process 2D Geometry
        if contents['has_2d']:
            messages.addMessage("\n--- Processing 2D Geometry ---")
            tool_2d = LoadHECRAS2DGeometry()
            
            # Determine which elements to load
            elements_2d = contents['2d_elements'][:]
            if include_cell_polygons:
                elements_2d.append("Mesh Cells (Polygons)")
            
            # Create parameters for 2D tool
            params_2d = [
                hdf_path,  # input HDF
                override_crs,  # override CRS
                elements_2d,  # elements to load
                None, None, None, None, None, None, None, None, None,  # output paths
                None,  # geodatabase (None to prevent tools from overriding our paths)
                False  # create_gdb
            ]
            
            mock_params_2d = [MockParam(p) for p in params_2d]
            
            # Set output paths with full naming convention for multi-plan processing
            # Map indices to element names
            element_indices = {
                3: ("2D Breaklines", "Breaklines2D"),
                4: ("2D Boundary Condition Lines", "BCLines2D"),
                5: ("Mesh Area Perimeters", "MeshPerimeters"),
                6: ("Mesh Cell Centers", "MeshCellCenters"),
                7: ("Mesh Cell Faces", "MeshCellFaces"),
                8: ("Mesh Cells (Polygons)", "MeshCellPolygons"),
                9: ("Pipe Conduits", "PipeConduits"),
                10: ("Pipe Nodes", "PipeNodes"),
                11: ("Pipe Networks", "PipeNetworks")
            }
            
            for idx, (element_name, fc_base) in element_indices.items():
                if element_name in elements_2d:
                    fc_name = f"{project_name}_Plan_{plan_number}_{fc_base}"
                    mock_params_2d[idx].valueAsText = os.path.join(fd_path, fc_name)
            
            # Execute 2D tool
            try:
                tool_2d.execute(mock_params_2d, messages)
            except Exception as e:
                messages.addWarning(f"Error processing 2D geometry: {e}")
                messages.addWarning("Continuing with remaining data...")
        
        # Process Results
        if contents['has_results'] and extract_all_results:
            messages.addMessage("\n--- Processing Results ---")
            tool_results = LoadHECRAS2DResults()
            
            # Create parameters for results tool
            params_results = [
                hdf_path,  # input HDF
                override_crs,  # override CRS
                ["Max WSE at Cell Centers", "Max Vel at Cell Faces"],  # results elements to load
                None,  # output max wse
                None,  # output max vel
                None,  # geodatabase (None to prevent tools from overriding our paths)
                False  # create_gdb
            ]
            
            mock_params_results = [MockParam(p) for p in params_results]
            
            # Set result output paths with full naming convention
            fc_name = f"{project_name}_Plan_{plan_number}_MaxWSE"
            mock_params_results[3].valueAsText = os.path.join(fd_path, fc_name)
            
            fc_name = f"{project_name}_Plan_{plan_number}_MaxVelocity"
            mock_params_results[4].valueAsText = os.path.join(fd_path, fc_name)
            
            # Execute results tool
            try:
                tool_results.execute(mock_params_results, messages)
            except Exception as e:
                messages.addWarning(f"Error processing results: {e}")
                messages.addWarning("Continuing with remaining data...")
        
        messages.addMessage(f"\nCompleted processing: {os.path.basename(hdf_path)}")
        return
    
    def getHelp(self, tool_name):
        """Return help documentation URL for the tool."""
        help_file = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 
                                "Doc", "RASCommander_Help.html")
        if os.path.exists(help_file):
            return f"file:///{help_file.replace(os.sep, '/')}#organize-hec-ras-project"
        return None
==================================================

File: c:\GH\ras-commander-hydro\Scripts\archydro\rc_utils.py
==================================================
# -*- coding: utf-8 -*-
"""
utils.py

Shared utility functions for RAS Commander tools.
These helpers use arcpy.Add* functions because they don't have access
to the `messages` object from the tool's `execute` method.
"""

import arcpy
import os
import re
import h5py
import numpy as np
from collections import defaultdict


def get_ras_projection_wkt(hdf_path_str: str) -> str or None:
    """
    Gets projection WKT from HDF file or an associated .prj file.
    This is a self-contained port of the HdfBase.get_projection function
    from the ras-commander library.
    """
    hdf_path = os.path.abspath(hdf_path_str)
    project_folder = os.path.dirname(hdf_path)
    wkt = None
    try:
        with h5py.File(hdf_path, 'r') as hdf_file:
            proj_wkt_attr = hdf_file.attrs.get("Projection")
            if proj_wkt_attr:
                if isinstance(proj_wkt_attr, (bytes, np.bytes_)):
                    wkt = proj_wkt_attr.decode("utf-8")
                    arcpy.AddMessage(f"Found projection in HDF file: {os.path.basename(hdf_path)}")
                    return wkt
    except Exception as e:
        arcpy.AddWarning(f"Could not read projection from HDF file attribute: {e}")
    if not wkt:
        try:
            rasmap_files = [f for f in os.listdir(project_folder) if f.lower().endswith(".rasmap")]
            if rasmap_files:
                rasmap_file_path = os.path.join(project_folder, rasmap_files[0])
                with open(rasmap_file_path, 'r', errors='ignore') as f:
                    content = f.read()
                proj_match = re.search(r'<RASProjectionFilename Filename="(.*?)"', content)
                if proj_match:
                    prj_filename = proj_match.group(1).replace('.\\', '')
                    proj_file = os.path.join(project_folder, prj_filename)
                    if os.path.exists(proj_file):
                        with open(proj_file, 'r') as f_prj:
                            wkt = f_prj.read().strip()
                            arcpy.AddMessage(f"Found projection in associated RASMapper file: {os.path.basename(proj_file)}")
                            return wkt
        except Exception as e:
            arcpy.AddWarning(f"Could not read projection from RASMapper file: {e}")
    return None


def polygonize_arcpy_optimized(line_geometries, sr):
    """
    Optimized polygon creation from line geometries using numpy arrays.
    """
    if not line_geometries:
        return None
    
    try:
        # Pre-process edges into numpy arrays for efficiency
        edge_coords = []
        edge_connections = defaultdict(list)
        tolerance = 1e-9
        
        # Extract all edge coordinates at once
        for line_idx, line in enumerate(line_geometries):
            if line is None or (hasattr(line, 'length') and line.length == 0):
                continue
            
            # Get line coordinates as numpy array
            part = line.getPart(0)
            if part.count >= 2:
                coords = np.array([[part.getObject(i).X, part.getObject(i).Y] 
                                 for i in range(part.count) if part.getObject(i)])
                
                if len(coords) >= 2:
                    edge_coords.append(coords)
                    
                    # Store connections using rounded coordinates for tolerance
                    start_key = tuple(np.round(coords[0], decimals=9))
                    end_key = tuple(np.round(coords[-1], decimals=9))
                    
                    edge_connections[start_key].append((end_key, line_idx, False))
                    edge_connections[end_key].append((start_key, line_idx, True))
        
        if not edge_coords:
            return None
        
        # Find starting point with exactly 2 connections (ideal for tracing)
        start_point = None
        for pt, connections in edge_connections.items():
            if len(connections) == 2:
                start_point = pt
                break
        
        if start_point is None:
            start_point = next(iter(edge_connections))
        
        # Trace polygon using optimized lookup
        visited = set()
        ring_coords = [start_point]
        current = start_point
        
        max_edges = len(edge_coords)
        edge_count = 0
        
        while edge_count < max_edges:
            found_next = False
            
            for next_point, edge_idx, is_reversed in edge_connections[current]:
                edge_key = (edge_idx, is_reversed)
                
                if edge_key not in visited:
                    visited.add(edge_key)
                    visited.add((edge_idx, not is_reversed))
                    
                    # Get edge coordinates
                    coords = edge_coords[edge_idx]
                    if is_reversed:
                        coords = coords[::-1]
                    
                    # Add intermediate points
                    if len(coords) > 2:
                        ring_coords.extend([tuple(c) for c in coords[1:-1]])
                    
                    ring_coords.append(next_point)
                    current = next_point
                    found_next = True
                    edge_count += 1
                    
                    # Check if closed
                    if current == start_point and len(ring_coords) > 3:
                        ring_coords = ring_coords[:-1]  # Remove duplicate
                        
                        # Convert to numpy array for efficient operations
                        ring_array = np.array(ring_coords)
                        
                        # Ensure clockwise orientation
                        if not is_clockwise_numpy(ring_array):
                            ring_array = ring_array[::-1]
                        
                        # Create polygon
                        arcpy_array = arcpy.Array([arcpy.Point(x, y) for x, y in ring_array])
                        return arcpy.Polygon(arcpy_array, sr)
                    break
            
            if not found_next:
                break
        
        # If trace failed, try to create from unique points
        if len(ring_coords) >= 3:
            # Remove duplicates while preserving order
            seen = set()
            unique_coords = []
            for coord in ring_coords:
                if coord not in seen:
                    seen.add(coord)
                    unique_coords.append(coord)
            
            if len(unique_coords) >= 3:
                ring_array = np.array(unique_coords)
                
                if not is_clockwise_numpy(ring_array):
                    ring_array = ring_array[::-1]
                
                arcpy_array = arcpy.Array([arcpy.Point(x, y) for x, y in ring_array])
                return arcpy.Polygon(arcpy_array, sr)
        
        return None
        
    except Exception as e:
        arcpy.AddWarning(f"Polygon construction failed: {e}")
        return None


def is_clockwise_numpy(coords):
    """Check if polygon coordinates are in clockwise order using numpy."""
    coords = np.asarray(coords)
    x = coords[:, 0]
    y = coords[:, 1]
    
    # Vectorized shoelace formula
    area = 0.5 * np.sum(x[:-1] * y[1:] - x[1:] * y[:-1])
    area += 0.5 * (x[-1] * y[0] - x[0] * y[-1])
    
    return area < 0


def get_polyline_centroid_vectorized(polyline):
    """Calculate centroid of polyline using vectorized operations."""
    try:
        part = polyline.getPart(0)
        coords = np.array([[part.getObject(i).X, part.getObject(i).Y] 
                          for i in range(part.count) if part.getObject(i)])
        
        if len(coords) < 2:
            return None
        
        # Vectorized segment calculations
        segments = coords[1:] - coords[:-1]
        lengths = np.sqrt(np.sum(segments**2, axis=1))
        
        # Segment midpoints
        midpoints = (coords[:-1] + coords[1:]) / 2.0
        
        # Weighted centroid
        total_length = np.sum(lengths)
        if total_length > 0:
            weighted_coords = np.sum(midpoints * lengths[:, np.newaxis], axis=0) / total_length
            return arcpy.Point(weighted_coords[0], weighted_coords[1])
        
        return None
        
    except Exception as e:
        arcpy.AddWarning(f"Error calculating centroid: {e}")
        return None


def cache_hdf_metadata(hdf_file):
    """Pre-cache HDF metadata for faster access."""
    hdf_cache = {
        'mesh_names': [],
        'mesh_metadata': {},
        'has_results': False,
        'simulation_start_time': None
    }
    
    # Get mesh names
    flow_areas_path = "Geometry/2D Flow Areas"
    if flow_areas_path in hdf_file and "Attributes" in hdf_file[flow_areas_path]:
        attributes = hdf_file[f"{flow_areas_path}/Attributes"][()]
        hdf_cache['mesh_names'] = [n.decode('utf-8', 'ignore').strip() 
                                       for n in attributes["Name"]]
        
        # Cache mesh metadata
        for mesh_name in hdf_cache['mesh_names']:
            base_path = f"{flow_areas_path}/{mesh_name}"
            metadata = {}
            
            # Cache dataset sizes
            if f"{base_path}/Cells Center Coordinate" in hdf_file:
                metadata['cell_count'] = len(hdf_file[f"{base_path}/Cells Center Coordinate"])
            
            if f"{base_path}/Faces FacePoint Indexes" in hdf_file:
                metadata['face_count'] = len(hdf_file[f"{base_path}/Faces FacePoint Indexes"])
            
            hdf_cache['mesh_metadata'][mesh_name] = metadata
    
    # Check for boundary condition lines
    bc_lines_path = "Geometry/Boundary Condition Lines"
    if bc_lines_path in hdf_file:
        hdf_cache['has_bc_lines'] = True
        hdf_cache['bc_lines_count'] = len(hdf_file[f"{bc_lines_path}/Attributes"][()])
    else:
        hdf_cache['has_bc_lines'] = False
    
    # Check for pipe network elements
    pipe_conduits_path = "Geometry/Pipe Conduits"
    if pipe_conduits_path in hdf_file:
        hdf_cache['has_pipe_conduits'] = True
        if 'Attributes' in hdf_file[pipe_conduits_path]:
            hdf_cache['pipe_conduits_count'] = len(
                hdf_file[f"{pipe_conduits_path}/Attributes"][()]
            )
    else:
        hdf_cache['has_pipe_conduits'] = False
        
    pipe_nodes_path = "Geometry/Pipe Nodes"
    if pipe_nodes_path in hdf_file:
        hdf_cache['has_pipe_nodes'] = True
        if 'Attributes' in hdf_file[pipe_nodes_path]:
            hdf_cache['pipe_nodes_count'] = len(
                hdf_file[f"{pipe_nodes_path}/Attributes"][()]
            )
    else:
        hdf_cache['has_pipe_nodes'] = False
    
    # Check for results and get simulation time
    plan_info = hdf_file.get("Plan Data/Plan Information")
    if plan_info and 'Simulation Start Time' in plan_info.attrs:
        from datetime import datetime
        time_str = plan_info.attrs['Simulation Start Time']
        hdf_cache['simulation_start_time'] = datetime.strptime(
            time_str.decode('utf-8'), "%d%b%Y %H:%M:%S"
        )
        hdf_cache['has_results'] = True
    
    return hdf_cache


def get_dynamic_fields_from_data(data_list):
    """Determines field definitions from dynamic attribute data."""
    if not data_list:
        return []
    
    # Get all unique field names and their types
    field_info = {}
    field_lengths = {}  # Track max length for text fields
    
    for record in data_list:
        for field_name, value in record.items():
            # Skip None or NaN values for type detection
            if value is None or (isinstance(value, (float, np.floating)) and np.isnan(value)):
                continue
                
            if field_name not in field_info:
                # Determine field type based on value
                if isinstance(value, (bool, np.bool_)):
                    field_info[field_name] = "SHORT"  # Use SHORT for boolean
                elif isinstance(value, (int, np.integer)):
                    field_info[field_name] = "LONG"
                elif isinstance(value, (float, np.floating)):
                    field_info[field_name] = "DOUBLE"
                else:
                    field_info[field_name] = "TEXT"
                    field_lengths[field_name] = 0
            
            # Track max length for text fields
            if field_info.get(field_name) == "TEXT" and value is not None:
                str_value = str(value)
                field_lengths[field_name] = max(field_lengths.get(field_name, 0), len(str_value))
    
    # Check for fields that might have been skipped due to all NaN values
    all_field_names = set()
    for record in data_list:
        all_field_names.update(record.keys())
    
    # Add any missing fields with default type
    for field_name in all_field_names:
        if field_name not in field_info:
            # Default to DOUBLE for numeric-sounding fields, TEXT otherwise
            if any(keyword in field_name.lower() for keyword in ['elevation', 'area', 'length', 'coefficient', 'offset']):
                field_info[field_name] = "DOUBLE"
            else:
                field_info[field_name] = "TEXT"
                field_lengths[field_name] = 50
    
    # Convert to list of tuples for field creation
    fields = []
    for name, ftype in field_info.items():
        if ftype == "TEXT":
            # Ensure minimum field length of 50, max 255
            length = min(max(field_lengths.get(name, 50), 50), 255)
            fields.append((name, ftype, length))
        else:
            fields.append((name, ftype))
    
    return fields


def setup_geodatabase_output(gdb_path, feature_dataset_name, spatial_reference, messages):
    """
    Sets up geodatabase and feature dataset for organized output.
    
    Args:
        gdb_path: Path to geodatabase
        feature_dataset_name: Name of feature dataset to create (can be None)
        spatial_reference: Spatial reference for the feature dataset
        messages: ArcGIS messages object
        
    Returns:
        Path to feature dataset or geodatabase
    """
    # Create geodatabase if it doesn't exist
    if not arcpy.Exists(gdb_path):
        gdb_folder = os.path.dirname(gdb_path)
        gdb_name = os.path.basename(gdb_path)
        arcpy.CreateFileGDB_management(gdb_folder, gdb_name)
        messages.addMessage(f"Created geodatabase: {gdb_path}")
    
    # Create feature dataset if name provided
    if feature_dataset_name:
        fd_path = os.path.join(gdb_path, feature_dataset_name)
        if not arcpy.Exists(fd_path):
            arcpy.CreateFeatureDataset_management(gdb_path, feature_dataset_name, spatial_reference)
            messages.addMessage(f"Created feature dataset: {feature_dataset_name}")
        return fd_path
    
    return gdb_path


def get_unique_fc_name(workspace, base_name):
    """
    Generates a unique feature class name in the workspace.
    
    Args:
        workspace: Geodatabase or feature dataset path
        base_name: Desired feature class name
        
    Returns:
        Unique feature class name
    """
    fc_name = base_name
    counter = 1
    while arcpy.Exists(os.path.join(workspace, fc_name)):
        fc_name = f"{base_name}_{counter}"
        counter += 1
    return fc_name


def add_feature_class_metadata(fc_path, description, source_file):
    """
    Add metadata to a feature class.
    
    Args:
        fc_path: Path to feature class
        description: Description of the feature class
        source_file: Source HDF file path
    """
    try:
        metadata = arcpy.metadata.Metadata(fc_path)
        metadata.description = description
        metadata.summary = f"Extracted from HEC-RAS file: {os.path.basename(source_file)}"
        metadata.tags = "HEC-RAS, Hydraulic Modeling, RAS Commander"
        metadata.save()
    except Exception as e:
        # Metadata operations may fail in some environments, don't stop execution
        arcpy.AddWarning(f"Could not add metadata to {os.path.basename(fc_path)}: {e}")


def apply_symbology_from_layer(fc_path, layer_file_path):
    """
    Apply symbology from a layer file to a feature class.
    
    Args:
        fc_path: Path to feature class
        layer_file_path: Path to .lyrx file with symbology
    """
    try:
        if arcpy.Exists(layer_file_path):
            arcpy.ApplySymbologyFromLayer_management(fc_path, layer_file_path)
    except Exception as e:
        # Symbology operations are optional, don't stop execution
        arcpy.AddWarning(f"Could not apply symbology: {e}")


def write_features_to_fc(output_fc, sr, geom_type, fields, data, geometries, messages):
    """Optimized feature writing with batch operations."""
    total_features = len(geometries)
    if total_features == 0:
        messages.addWarningMessage(f"No features found for {os.path.basename(output_fc)}. Layer will not be created.")
        return

    messages.addMessage(f"Creating feature class: {os.path.basename(output_fc)} ({total_features} features)")
    
    try:
        output_path, output_name = os.path.split(output_fc)
        if arcpy.Exists(output_fc):
            arcpy.management.Delete(output_fc)
        arcpy.management.CreateFeatureclass(output_path, output_name, geom_type, spatial_reference=sr)
        
        field_names = []
        for field_def in fields:
            field_name = field_def[0]
            field_type = field_def[1]
            
            if field_type == "DATE":
                arcpy.management.AddField(output_fc, field_name, "DATE")
            elif field_type == "TEXT" and len(field_def) > 2:
                # Use the calculated field length for text fields
                field_length = field_def[2]
                arcpy.management.AddField(output_fc, field_name, field_type, field_length=field_length)
            else:
                arcpy.management.AddField(output_fc, field_name, field_type)
            field_names.append(field_name)
            
    except Exception as e:
        messages.addErrorMessage(f"Failed to create output feature class {output_fc}: {e}")
        return
    
    try:
        field_names_with_shape = ["SHAPE@"] + field_names
        features_inserted = 0
        
        # Prepare all rows at once
        all_rows = []
        missing_fields_warned = set()  # Track which fields we've already warned about
        
        for i, geom in enumerate(geometries):
            if geom is None or geom.pointCount == 0:
                continue
            
            row_data = data[i]
            row_values = []
            for name in field_names:
                # Use the exact field name from the data, accounting for any cleanup
                value = row_data.get(name)
                if value is None and name not in row_data:
                    # Only warn once per field across all rows
                    if name not in missing_fields_warned:
                        messages.addWarning(f"Field '{name}' not found in data. Available fields: {list(row_data.keys())}")
                        missing_fields_warned.add(name)
                    value = None  # Use None for missing fields
                
                # Handle special numpy types and NaN values
                if value is not None:
                    # Convert numpy NaN to None for proper NULL handling
                    if isinstance(value, (float, np.floating)) and np.isnan(value):
                        value = None
                    # Convert numpy types to Python types
                    elif isinstance(value, np.integer):
                        value = int(value)
                    elif isinstance(value, np.floating):
                        value = float(value)
                    elif isinstance(value, np.bool_):
                        value = int(value)  # Convert bool to 0/1 for SHORT field
                
                row_values.append(value)
            all_rows.append([geom] + row_values)
        
        # Batch insert with larger chunks
        batch_size = 10000
        with arcpy.da.InsertCursor(output_fc, field_names_with_shape) as cursor:
            for i in range(0, len(all_rows), batch_size):
                batch = all_rows[i:i + batch_size]
                for row in batch:
                    cursor.insertRow(row)
                features_inserted += len(batch)
                
                if features_inserted % 50000 == 0:
                    messages.addMessage(f"  Inserted {features_inserted}/{len(all_rows)} features...")
        
        messages.addMessage(f"Successfully created {os.path.basename(output_fc)} with {features_inserted} features.")
        
    except Exception as e:
        # If the error is about a missing field, show more diagnostic info
        if "Cannot find field" in str(e) or "not found in data" in str(e):
            messages.addErrorMessage(f"Field mapping error for {output_fc}:")
            messages.addErrorMessage(f"  Expected fields: {field_names}")
            if all_rows and len(all_rows) > 0:
                sample_data = data[0] if data else {}
                messages.addErrorMessage(f"  Available fields in data: {list(sample_data.keys())}")
            messages.addErrorMessage(f"  Original error: {e}")
        else:
            messages.addErrorMessage(f"Failed during feature creation for {output_fc}: {e}")


def extract_project_and_plan_info(hdf_path):
    """
    Extract project name and plan number from HDF file path.
    
    Args:
        hdf_path: Path to HDF file (e.g., "BaldEagleDamBrk.p07.hdf")
        
    Returns:
        tuple: (project_name, plan_number, base_name)
        Example: ("BaldEagleDamBrk", "07", "BaldEagleDamBrk.p07")
    """
    filename = os.path.basename(hdf_path)
    base_name = os.path.splitext(filename)[0]
    
    # Try to match pattern: ProjectName.pXX
    match = re.match(r'^(.+?)\.p(\d{2})$', base_name)
    
    if match:
        project_name = match.group(1)
        plan_number = match.group(2)
    else:
        # Fallback: use whole base name as project
        project_name = base_name
        plan_number = "00"
    
    return project_name, plan_number, base_name


def create_geodatabase_from_hdf(hdf_path, messages=None):
    """
    Create a geodatabase path based on HDF file name.
    
    Args:
        hdf_path: Path to HDF file
        messages: Optional messages object for logging
        
    Returns:
        Path to geodatabase
    """
    project_name, plan_number, base_name = extract_project_and_plan_info(hdf_path)
    
    # Create geodatabase name
    gdb_name = f"{base_name}.gdb"
    gdb_folder = os.path.dirname(hdf_path)
    gdb_path = os.path.join(gdb_folder, gdb_name)
    
    # Create if doesn't exist
    if not arcpy.Exists(gdb_path):
        arcpy.CreateFileGDB_management(gdb_folder, gdb_name)
        if messages:
            messages.addMessage(f"Created geodatabase: {gdb_path}")
    
    return gdb_path


def get_feature_dataset_name(hdf_path):
    """
    Get feature dataset name based on project and plan.
    
    Args:
        hdf_path: Path to HDF file
        
    Returns:
        Feature dataset name (e.g., "BaldEagleDamBrk_Plan07")
    """
    project_name, plan_number, _ = extract_project_and_plan_info(hdf_path)
    return f"{project_name}_Plan{plan_number}"


def get_feature_class_name(base_name, project_name, plan_number):
    """
    Get feature class name with project and plan info.
    
    Args:
        base_name: Base feature class name (e.g., "CrossSections")
        project_name: Project name
        plan_number: Plan number
        
    Returns:
        Full feature class name
    """
    return f"{base_name}_{project_name}_Plan{plan_number}"
==================================================

